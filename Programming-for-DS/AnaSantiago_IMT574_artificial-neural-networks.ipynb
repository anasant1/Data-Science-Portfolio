{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e6e7aa",
   "metadata": {},
   "source": [
    "# 9.6 Assignment: Artificial Neural Networks\n",
    "\n",
    "The following ANN uses the Steel Plages Faults Dataset which has the following attributes:\n",
    "\n",
    "DEPENDENT VARIABLES (7 types of steel plates faults)\n",
    "- Pastry\n",
    "\n",
    "- Z_scratch\n",
    "\n",
    "- K_scratch\n",
    "\n",
    "- Stains\n",
    "\n",
    "- Dirtiness\n",
    "\n",
    "- Bumps\n",
    "\n",
    "- Other_Faults\n",
    "\n",
    "INDEPENDENT VARIABLES (all numeric except the 2 noted below)\n",
    "- X_Minimum\t\n",
    "\n",
    "- X_Maximum\t\n",
    "\n",
    "- Y_Minimum\n",
    "\n",
    "- Y_Maximum\n",
    "\n",
    "- Pixels_Areas\n",
    "\n",
    "- X_Perimeter\n",
    "\n",
    "- Y_Perimeter\n",
    "\n",
    "- Sum_of_Luminosity\n",
    "\n",
    "- Minimum_of_Luminosity\n",
    "\n",
    "- Maximum_of_Luminosity\n",
    "\n",
    "- Length_of_Conveyer\n",
    "\n",
    "- TypeOfSteel_A300 (categorical)\n",
    "\n",
    "- TypeOfSteel_A400 (categorical)\n",
    "\n",
    "- Steel_Plate_Thickness\n",
    "\n",
    "- Edges_Index\n",
    "\n",
    "- Empty_Index\n",
    "\n",
    "- Square_Index\n",
    "\n",
    "- Outside_X_Index\n",
    "\n",
    "- Edges_X_Index\n",
    "\n",
    "- Edges_Y_Index\n",
    "\n",
    "- Outside_Global_Index\n",
    "\n",
    "- LogOfAreas\n",
    "\n",
    "- Log_X_Index\n",
    "\n",
    "- Log_Y_Index\n",
    "\n",
    "- Orientation_Index\n",
    "\n",
    "- Luminosity_Index\n",
    "\n",
    "- SigmoidOfAreas\n",
    "\n",
    "### TABLE OF CONTENTS:\n",
    "[Data processing](#Data-processing)\n",
    "\n",
    "[ANN](#ANN)\n",
    "\n",
    "[Conclusions](#Conclusions)\n",
    "\n",
    "### Objectives\n",
    "- build a neural network to see how well it can predict the type of faults in steel plates from numerical attributes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42288acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbae1e",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cff5d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_min</th>\n",
       "      <th>y_max</th>\n",
       "      <th>pixels_areas</th>\n",
       "      <th>x_perim</th>\n",
       "      <th>y_perim</th>\n",
       "      <th>sum_lumin</th>\n",
       "      <th>min_lumin</th>\n",
       "      <th>max_lumin</th>\n",
       "      <th>...</th>\n",
       "      <th>orientation_idx</th>\n",
       "      <th>lumin_idx</th>\n",
       "      <th>sigmoid_areas</th>\n",
       "      <th>pastry</th>\n",
       "      <th>z_scratch</th>\n",
       "      <th>k_scratch</th>\n",
       "      <th>stains</th>\n",
       "      <th>dirtiness</th>\n",
       "      <th>bumps</th>\n",
       "      <th>other_faults</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>270900</td>\n",
       "      <td>270944</td>\n",
       "      <td>267</td>\n",
       "      <td>17</td>\n",
       "      <td>44</td>\n",
       "      <td>24220</td>\n",
       "      <td>76</td>\n",
       "      <td>108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8182</td>\n",
       "      <td>-0.2913</td>\n",
       "      <td>0.5822</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>645</td>\n",
       "      <td>651</td>\n",
       "      <td>2538079</td>\n",
       "      <td>2538108</td>\n",
       "      <td>108</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>11397</td>\n",
       "      <td>84</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>-0.1756</td>\n",
       "      <td>0.2984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>829</td>\n",
       "      <td>835</td>\n",
       "      <td>1553913</td>\n",
       "      <td>1553931</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>7972</td>\n",
       "      <td>99</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>-0.1228</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853</td>\n",
       "      <td>860</td>\n",
       "      <td>369370</td>\n",
       "      <td>369415</td>\n",
       "      <td>176</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>18996</td>\n",
       "      <td>99</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>-0.1568</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1289</td>\n",
       "      <td>1306</td>\n",
       "      <td>498078</td>\n",
       "      <td>498335</td>\n",
       "      <td>2409</td>\n",
       "      <td>60</td>\n",
       "      <td>260</td>\n",
       "      <td>246930</td>\n",
       "      <td>37</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9338</td>\n",
       "      <td>-0.1992</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_min  x_max    y_min    y_max  pixels_areas  x_perim  y_perim  sum_lumin  \\\n",
       "0     42     50   270900   270944           267       17       44      24220   \n",
       "1    645    651  2538079  2538108           108       10       30      11397   \n",
       "2    829    835  1553913  1553931            71        8       19       7972   \n",
       "3    853    860   369370   369415           176       13       45      18996   \n",
       "4   1289   1306   498078   498335          2409       60      260     246930   \n",
       "\n",
       "   min_lumin  max_lumin  ...  orientation_idx  lumin_idx  sigmoid_areas  \\\n",
       "0         76        108  ...           0.8182    -0.2913         0.5822   \n",
       "1         84        123  ...           0.7931    -0.1756         0.2984   \n",
       "2         99        125  ...           0.6667    -0.1228         0.2150   \n",
       "3         99        126  ...           0.8444    -0.1568         0.5212   \n",
       "4         37        126  ...           0.9338    -0.1992         1.0000   \n",
       "\n",
       "   pastry  z_scratch  k_scratch  stains  dirtiness  bumps  other_faults  \n",
       "0       1          0          0       0          0      0             0  \n",
       "1       1          0          0       0          0      0             0  \n",
       "2       1          0          0       0          0      0             0  \n",
       "3       1          0          0       0          0      0             0  \n",
       "4       1          0          0       0          0      0             0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "col_names = ['x_min', 'x_max', 'y_min', 'y_max', 'pixels_areas', 'x_perim', 'y_perim', 'sum_lumin', 'min_lumin',\n",
    "            'max_lumin', 'len_conveyer', 'type_a300', 'type_a400', 'thickness', 'edges_idx', 'empty_idx',\n",
    "            'square_idx', 'outside_x_idx', 'edges_x_idx', 'edges_y_idx', 'outside_global_idx', \n",
    "            'log_areas', 'log_x_idx', 'log_y_idx', 'orientation_idx', 'lumin_idx', 'sigmoid_areas', \n",
    "             'pastry', 'z_scratch', 'k_scratch', 'stains', 'dirtiness', 'bumps', 'other_faults',]\n",
    "\n",
    "data = pd.read_csv('Assignment 9 dataset-Faults.nna', sep = '\\t', names = col_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533f07a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check df shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0124ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pastry                False\n",
       "z_scratch             False\n",
       "k_scratch             False\n",
       "stains                False\n",
       "dirtiness             False\n",
       "bumps                 False\n",
       "other_faults          False\n",
       "x_min                 False\n",
       "x_max                 False\n",
       "y_min                 False\n",
       "y_max                 False\n",
       "pixels_areas          False\n",
       "x_perim               False\n",
       "y_perim               False\n",
       "sum_lumin             False\n",
       "min_lumin             False\n",
       "max_lumin             False\n",
       "len_conveyer          False\n",
       "type_a300             False\n",
       "type_a400             False\n",
       "thickness             False\n",
       "edges_idx             False\n",
       "empty_idx             False\n",
       "square_idx            False\n",
       "outside_x_idx         False\n",
       "edges_x_idx           False\n",
       "edges_y_idx           False\n",
       "outside_global_idx    False\n",
       "log_areas             False\n",
       "log_x_idx             False\n",
       "log_y_idx             False\n",
       "orientation_idx       False\n",
       "lumin_idx             False\n",
       "sigmoid_areas         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for nulls\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40847ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find empty cells\n",
    "def find_empty(df, col_list):\n",
    "    empty_cells = []\n",
    "    for column in col_list:\n",
    "        for index, value in df[column].items():\n",
    "            if value == ' ':\n",
    "                empty_cells.append((index, column, value))\n",
    "    return empty_cells\n",
    "\n",
    "find_empty(data, col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36877220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pastry</th>\n",
       "      <th>z_scratch</th>\n",
       "      <th>k_scratch</th>\n",
       "      <th>stains</th>\n",
       "      <th>dirtiness</th>\n",
       "      <th>bumps</th>\n",
       "      <th>other_faults</th>\n",
       "      <th>x_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_min</th>\n",
       "      <th>...</th>\n",
       "      <th>outside_x_idx</th>\n",
       "      <th>edges_x_idx</th>\n",
       "      <th>edges_y_idx</th>\n",
       "      <th>outside_global_idx</th>\n",
       "      <th>log_areas</th>\n",
       "      <th>log_x_idx</th>\n",
       "      <th>log_y_idx</th>\n",
       "      <th>orientation_idx</th>\n",
       "      <th>lumin_idx</th>\n",
       "      <th>sigmoid_areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1.941000e+03</td>\n",
       "      <td>1.941000e+03</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1.941000e+03</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "      <td>1941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>571.136012</td>\n",
       "      <td>617.964451</td>\n",
       "      <td>1.650685e+06</td>\n",
       "      <td>1.650739e+06</td>\n",
       "      <td>1893.878413</td>\n",
       "      <td>111.855229</td>\n",
       "      <td>82.965997</td>\n",
       "      <td>2.063121e+05</td>\n",
       "      <td>84.548686</td>\n",
       "      <td>130.193715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083288</td>\n",
       "      <td>-0.131305</td>\n",
       "      <td>0.585420</td>\n",
       "      <td>0.081401</td>\n",
       "      <td>0.097888</td>\n",
       "      <td>0.201443</td>\n",
       "      <td>0.037094</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>0.207110</td>\n",
       "      <td>0.346728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>520.690671</td>\n",
       "      <td>497.627410</td>\n",
       "      <td>1.774578e+06</td>\n",
       "      <td>1.774590e+06</td>\n",
       "      <td>5168.459560</td>\n",
       "      <td>301.209187</td>\n",
       "      <td>426.482879</td>\n",
       "      <td>5.122936e+05</td>\n",
       "      <td>32.134276</td>\n",
       "      <td>18.690992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500868</td>\n",
       "      <td>0.148767</td>\n",
       "      <td>0.339452</td>\n",
       "      <td>0.273521</td>\n",
       "      <td>0.297239</td>\n",
       "      <td>0.401181</td>\n",
       "      <td>0.189042</td>\n",
       "      <td>0.165973</td>\n",
       "      <td>0.405339</td>\n",
       "      <td>0.476051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.712000e+03</td>\n",
       "      <td>6.724000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991000</td>\n",
       "      <td>-0.998900</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>4.712530e+05</td>\n",
       "      <td>4.712810e+05</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.522000e+03</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333300</td>\n",
       "      <td>-0.195000</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>435.000000</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>1.204128e+06</td>\n",
       "      <td>1.204136e+06</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.920200e+04</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>-0.133000</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1053.000000</td>\n",
       "      <td>1072.000000</td>\n",
       "      <td>2.183073e+06</td>\n",
       "      <td>2.183084e+06</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>8.301100e+04</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>-0.066600</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1705.000000</td>\n",
       "      <td>1713.000000</td>\n",
       "      <td>1.298766e+07</td>\n",
       "      <td>1.298769e+07</td>\n",
       "      <td>152655.000000</td>\n",
       "      <td>10449.000000</td>\n",
       "      <td>18152.000000</td>\n",
       "      <td>1.159141e+07</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.642100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pastry    z_scratch     k_scratch        stains      dirtiness  \\\n",
       "count  1941.000000  1941.000000  1.941000e+03  1.941000e+03    1941.000000   \n",
       "mean    571.136012   617.964451  1.650685e+06  1.650739e+06    1893.878413   \n",
       "std     520.690671   497.627410  1.774578e+06  1.774590e+06    5168.459560   \n",
       "min       0.000000     4.000000  6.712000e+03  6.724000e+03       2.000000   \n",
       "25%      51.000000   192.000000  4.712530e+05  4.712810e+05      84.000000   \n",
       "50%     435.000000   467.000000  1.204128e+06  1.204136e+06     174.000000   \n",
       "75%    1053.000000  1072.000000  2.183073e+06  2.183084e+06     822.000000   \n",
       "max    1705.000000  1713.000000  1.298766e+07  1.298769e+07  152655.000000   \n",
       "\n",
       "              bumps  other_faults         x_min        x_max        y_min  \\\n",
       "count   1941.000000   1941.000000  1.941000e+03  1941.000000  1941.000000   \n",
       "mean     111.855229     82.965997  2.063121e+05    84.548686   130.193715   \n",
       "std      301.209187    426.482879  5.122936e+05    32.134276    18.690992   \n",
       "min        2.000000      1.000000  2.500000e+02     0.000000    37.000000   \n",
       "25%       15.000000     13.000000  9.522000e+03    63.000000   124.000000   \n",
       "50%       26.000000     25.000000  1.920200e+04    90.000000   127.000000   \n",
       "75%       84.000000     83.000000  8.301100e+04   106.000000   140.000000   \n",
       "max    10449.000000  18152.000000  1.159141e+07   203.000000   253.000000   \n",
       "\n",
       "       ...  outside_x_idx  edges_x_idx  edges_y_idx  outside_global_idx  \\\n",
       "count  ...    1941.000000  1941.000000  1941.000000         1941.000000   \n",
       "mean   ...       0.083288    -0.131305     0.585420            0.081401   \n",
       "std    ...       0.500868     0.148767     0.339452            0.273521   \n",
       "min    ...      -0.991000    -0.998900     0.119000            0.000000   \n",
       "25%    ...      -0.333300    -0.195000     0.248200            0.000000   \n",
       "50%    ...       0.095200    -0.133000     0.506300            0.000000   \n",
       "75%    ...       0.511600    -0.066600     0.999800            0.000000   \n",
       "max    ...       0.991700     0.642100     1.000000            1.000000   \n",
       "\n",
       "         log_areas    log_x_idx    log_y_idx  orientation_idx    lumin_idx  \\\n",
       "count  1941.000000  1941.000000  1941.000000      1941.000000  1941.000000   \n",
       "mean      0.097888     0.201443     0.037094         0.028336     0.207110   \n",
       "std       0.297239     0.401181     0.189042         0.165973     0.405339   \n",
       "min       0.000000     0.000000     0.000000         0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000         0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000         0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000         0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000         1.000000     1.000000   \n",
       "\n",
       "       sigmoid_areas  \n",
       "count    1941.000000  \n",
       "mean        0.346728  \n",
       "std         0.476051  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ce75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize standard scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a29b6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop categorical attributes\n",
    "data = data.drop(columns = ['type_a300', 'type_a400'])\n",
    "\n",
    "#set input and output features\n",
    "X = data.iloc[:, :25]\n",
    "X = scaler.fit_transform(X)\n",
    "#first seven features will be output\n",
    "y = data.iloc[:, 25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe0f785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pastry</th>\n",
       "      <th>z_scratch</th>\n",
       "      <th>k_scratch</th>\n",
       "      <th>stains</th>\n",
       "      <th>dirtiness</th>\n",
       "      <th>bumps</th>\n",
       "      <th>other_faults</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1941 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pastry  z_scratch  k_scratch  stains  dirtiness  bumps  other_faults\n",
       "0          1          0          0       0          0      0             0\n",
       "1          1          0          0       0          0      0             0\n",
       "2          1          0          0       0          0      0             0\n",
       "3          1          0          0       0          0      0             0\n",
       "4          1          0          0       0          0      0             0\n",
       "...      ...        ...        ...     ...        ...    ...           ...\n",
       "1936       0          0          0       0          0      0             1\n",
       "1937       0          0          0       0          0      0             1\n",
       "1938       0          0          0       0          0      0             1\n",
       "1939       0          0          0       0          0      0             1\n",
       "1940       0          0          0       0          0      0             1\n",
       "\n",
       "[1941 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine y\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2281917",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f88a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert df features to numpy arrays\n",
    "X_numpy = X.values if isinstance(X, pd.DataFrame) else X\n",
    "y_numpy = y.values if isinstance(y, pd.DataFrame) else y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ac3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size = 0.3, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d12ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert split arrays to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbdb9be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25, out_features=7, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize model w/ 2 layers with rectified linear unit (ReLU) activation function for speed, and sigmoid function\n",
    "#for mapping probability for multiclass problem, 25 input variables, 7 neurons\n",
    "neural_network = nn.Sequential(\n",
    "                 nn.Linear(25, 7), \n",
    "                 nn.ReLU(), \n",
    "                 nn.Linear(7,7),  \n",
    "                 nn.Sigmoid())\n",
    "\n",
    "neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c250f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function as binary cross entropy (BCELoss)\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(neural_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d20be3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss was: 0.4454731047153473\n",
      "epoch: 2, loss was: 0.44378459453582764\n",
      "epoch: 3, loss was: 0.44211891293525696\n",
      "epoch: 4, loss was: 0.44047263264656067\n",
      "epoch: 5, loss was: 0.4388485252857208\n",
      "epoch: 6, loss was: 0.43724486231803894\n",
      "epoch: 7, loss was: 0.4356614053249359\n",
      "epoch: 8, loss was: 0.4340986907482147\n",
      "epoch: 9, loss was: 0.4325554370880127\n",
      "epoch: 10, loss was: 0.43103185296058655\n",
      "epoch: 11, loss was: 0.4295293390750885\n",
      "epoch: 12, loss was: 0.42804640531539917\n",
      "epoch: 13, loss was: 0.42658179998397827\n",
      "epoch: 14, loss was: 0.4251358211040497\n",
      "epoch: 15, loss was: 0.42371082305908203\n",
      "epoch: 16, loss was: 0.4223022758960724\n",
      "epoch: 17, loss was: 0.4209120571613312\n",
      "epoch: 18, loss was: 0.41954129934310913\n",
      "epoch: 19, loss was: 0.41818967461586\n",
      "epoch: 20, loss was: 0.4168568551540375\n",
      "epoch: 21, loss was: 0.41554248332977295\n",
      "epoch: 22, loss was: 0.41424688696861267\n",
      "epoch: 23, loss was: 0.41296935081481934\n",
      "epoch: 24, loss was: 0.41170814633369446\n",
      "epoch: 25, loss was: 0.4104633927345276\n",
      "epoch: 26, loss was: 0.40923452377319336\n",
      "epoch: 27, loss was: 0.40802350640296936\n",
      "epoch: 28, loss was: 0.40682920813560486\n",
      "epoch: 29, loss was: 0.40565091371536255\n",
      "epoch: 30, loss was: 0.40448856353759766\n",
      "epoch: 31, loss was: 0.4033416211605072\n",
      "epoch: 32, loss was: 0.40221020579338074\n",
      "epoch: 33, loss was: 0.4010940194129944\n",
      "epoch: 34, loss was: 0.39999231696128845\n",
      "epoch: 35, loss was: 0.39890506863594055\n",
      "epoch: 36, loss was: 0.3978319466114044\n",
      "epoch: 37, loss was: 0.39677247405052185\n",
      "epoch: 38, loss was: 0.3957267999649048\n",
      "epoch: 39, loss was: 0.3946938216686249\n",
      "epoch: 40, loss was: 0.39367353916168213\n",
      "epoch: 41, loss was: 0.39266595244407654\n",
      "epoch: 42, loss was: 0.3916705548763275\n",
      "epoch: 43, loss was: 0.3906870484352112\n",
      "epoch: 44, loss was: 0.38971540331840515\n",
      "epoch: 45, loss was: 0.38875502347946167\n",
      "epoch: 46, loss was: 0.3878054916858673\n",
      "epoch: 47, loss was: 0.38686713576316833\n",
      "epoch: 48, loss was: 0.3859395980834961\n",
      "epoch: 49, loss was: 0.3850206732749939\n",
      "epoch: 50, loss was: 0.38411182165145874\n",
      "epoch: 51, loss was: 0.383213073015213\n",
      "epoch: 52, loss was: 0.38232383131980896\n",
      "epoch: 53, loss was: 0.3814440667629242\n",
      "epoch: 54, loss was: 0.38057392835617065\n",
      "epoch: 55, loss was: 0.37971293926239014\n",
      "epoch: 56, loss was: 0.3788612484931946\n",
      "epoch: 57, loss was: 0.3780176639556885\n",
      "epoch: 58, loss was: 0.37718114256858826\n",
      "epoch: 59, loss was: 0.37635302543640137\n",
      "epoch: 60, loss was: 0.3755333423614502\n",
      "epoch: 61, loss was: 0.3747216463088989\n",
      "epoch: 62, loss was: 0.3739180862903595\n",
      "epoch: 63, loss was: 0.3731207549571991\n",
      "epoch: 64, loss was: 0.3723295331001282\n",
      "epoch: 65, loss was: 0.37154579162597656\n",
      "epoch: 66, loss was: 0.37076953053474426\n",
      "epoch: 67, loss was: 0.3700006306171417\n",
      "epoch: 68, loss was: 0.36923864483833313\n",
      "epoch: 69, loss was: 0.36848336458206177\n",
      "epoch: 70, loss was: 0.3677348494529724\n",
      "epoch: 71, loss was: 0.3669931888580322\n",
      "epoch: 72, loss was: 0.36625784635543823\n",
      "epoch: 73, loss was: 0.36552855372428894\n",
      "epoch: 74, loss was: 0.3648054897785187\n",
      "epoch: 75, loss was: 0.364087849855423\n",
      "epoch: 76, loss was: 0.3633763790130615\n",
      "epoch: 77, loss was: 0.3626706004142761\n",
      "epoch: 78, loss was: 0.361971914768219\n",
      "epoch: 79, loss was: 0.36127832531929016\n",
      "epoch: 80, loss was: 0.36059048771858215\n",
      "epoch: 81, loss was: 0.3599083125591278\n",
      "epoch: 82, loss was: 0.3592316210269928\n",
      "epoch: 83, loss was: 0.35856011509895325\n",
      "epoch: 84, loss was: 0.35789331793785095\n",
      "epoch: 85, loss was: 0.3572314381599426\n",
      "epoch: 86, loss was: 0.3565745949745178\n",
      "epoch: 87, loss was: 0.35592272877693176\n",
      "epoch: 88, loss was: 0.35527586936950684\n",
      "epoch: 89, loss was: 0.3546338677406311\n",
      "epoch: 90, loss was: 0.3539969027042389\n",
      "epoch: 91, loss was: 0.3533647954463959\n",
      "epoch: 92, loss was: 0.3527373969554901\n",
      "epoch: 93, loss was: 0.3521146774291992\n",
      "epoch: 94, loss was: 0.35149654746055603\n",
      "epoch: 95, loss was: 0.35088106989860535\n",
      "epoch: 96, loss was: 0.35026872158050537\n",
      "epoch: 97, loss was: 0.3496610224246979\n",
      "epoch: 98, loss was: 0.34905755519866943\n",
      "epoch: 99, loss was: 0.34845829010009766\n",
      "epoch: 100, loss was: 0.3478633761405945\n",
      "epoch: 101, loss was: 0.3472726345062256\n",
      "epoch: 102, loss was: 0.34668606519699097\n",
      "epoch: 103, loss was: 0.3461034595966339\n",
      "epoch: 104, loss was: 0.34552502632141113\n",
      "epoch: 105, loss was: 0.3449508249759674\n",
      "epoch: 106, loss was: 0.34438061714172363\n",
      "epoch: 107, loss was: 0.3438142240047455\n",
      "epoch: 108, loss was: 0.3432517349720001\n",
      "epoch: 109, loss was: 0.34269315004348755\n",
      "epoch: 110, loss was: 0.3421384394168854\n",
      "epoch: 111, loss was: 0.3415873348712921\n",
      "epoch: 112, loss was: 0.3410405218601227\n",
      "epoch: 113, loss was: 0.34049689769744873\n",
      "epoch: 114, loss was: 0.3399572968482971\n",
      "epoch: 115, loss was: 0.33942145109176636\n",
      "epoch: 116, loss was: 0.3388892114162445\n",
      "epoch: 117, loss was: 0.33836063742637634\n",
      "epoch: 118, loss was: 0.3378356099128723\n",
      "epoch: 119, loss was: 0.3373144268989563\n",
      "epoch: 120, loss was: 0.3367975056171417\n",
      "epoch: 121, loss was: 0.3362841308116913\n",
      "epoch: 122, loss was: 0.33577457070350647\n",
      "epoch: 123, loss was: 0.3352685272693634\n",
      "epoch: 124, loss was: 0.33476606011390686\n",
      "epoch: 125, loss was: 0.33426743745803833\n",
      "epoch: 126, loss was: 0.3337722420692444\n",
      "epoch: 127, loss was: 0.3332805633544922\n",
      "epoch: 128, loss was: 0.3327923119068146\n",
      "epoch: 129, loss was: 0.3323076665401459\n",
      "epoch: 130, loss was: 0.3318265378475189\n",
      "epoch: 131, loss was: 0.33134904503822327\n",
      "epoch: 132, loss was: 0.3308752179145813\n",
      "epoch: 133, loss was: 0.33040499687194824\n",
      "epoch: 134, loss was: 0.32993844151496887\n",
      "epoch: 135, loss was: 0.3294752240180969\n",
      "epoch: 136, loss was: 0.3290155231952667\n",
      "epoch: 137, loss was: 0.32855933904647827\n",
      "epoch: 138, loss was: 0.328106552362442\n",
      "epoch: 139, loss was: 0.3276570439338684\n",
      "epoch: 140, loss was: 0.3272112011909485\n",
      "epoch: 141, loss was: 0.3267689347267151\n",
      "epoch: 142, loss was: 0.3263293504714966\n",
      "epoch: 143, loss was: 0.3258924186229706\n",
      "epoch: 144, loss was: 0.325457900762558\n",
      "epoch: 145, loss was: 0.32502660155296326\n",
      "epoch: 146, loss was: 0.3245985805988312\n",
      "epoch: 147, loss was: 0.3241739273071289\n",
      "epoch: 148, loss was: 0.32375258207321167\n",
      "epoch: 149, loss was: 0.32333478331565857\n",
      "epoch: 150, loss was: 0.3229203224182129\n",
      "epoch: 151, loss was: 0.3225090801715851\n",
      "epoch: 152, loss was: 0.3221014142036438\n",
      "epoch: 153, loss was: 0.3216969668865204\n",
      "epoch: 154, loss was: 0.321295827627182\n",
      "epoch: 155, loss was: 0.3208976089954376\n",
      "epoch: 156, loss was: 0.3205006718635559\n",
      "epoch: 157, loss was: 0.32010406255722046\n",
      "epoch: 158, loss was: 0.31971025466918945\n",
      "epoch: 159, loss was: 0.3193160891532898\n",
      "epoch: 160, loss was: 0.31892332434654236\n",
      "epoch: 161, loss was: 0.3185337483882904\n",
      "epoch: 162, loss was: 0.3181475102901459\n",
      "epoch: 163, loss was: 0.31776413321495056\n",
      "epoch: 164, loss was: 0.31738221645355225\n",
      "epoch: 165, loss was: 0.31699836254119873\n",
      "epoch: 166, loss was: 0.3166158199310303\n",
      "epoch: 167, loss was: 0.3162362277507782\n",
      "epoch: 168, loss was: 0.315859854221344\n",
      "epoch: 169, loss was: 0.3154867887496948\n",
      "epoch: 170, loss was: 0.31511390209198\n",
      "epoch: 171, loss was: 0.31474271416664124\n",
      "epoch: 172, loss was: 0.31437399983406067\n",
      "epoch: 173, loss was: 0.31400266289711\n",
      "epoch: 174, loss was: 0.3136339485645294\n",
      "epoch: 175, loss was: 0.3132682144641876\n",
      "epoch: 176, loss was: 0.31290552020072937\n",
      "epoch: 177, loss was: 0.31254610419273376\n",
      "epoch: 178, loss was: 0.3121901750564575\n",
      "epoch: 179, loss was: 0.31183746457099915\n",
      "epoch: 180, loss was: 0.3114878833293915\n",
      "epoch: 181, loss was: 0.3111417889595032\n",
      "epoch: 182, loss was: 0.3107988238334656\n",
      "epoch: 183, loss was: 0.3104594051837921\n",
      "epoch: 184, loss was: 0.3101246953010559\n",
      "epoch: 185, loss was: 0.3097921907901764\n",
      "epoch: 186, loss was: 0.30946069955825806\n",
      "epoch: 187, loss was: 0.3091312646865845\n",
      "epoch: 188, loss was: 0.30880501866340637\n",
      "epoch: 189, loss was: 0.3084815740585327\n",
      "epoch: 190, loss was: 0.30816105008125305\n",
      "epoch: 191, loss was: 0.30784350633621216\n",
      "epoch: 192, loss was: 0.30752894282341003\n",
      "epoch: 193, loss was: 0.3072173297405243\n",
      "epoch: 194, loss was: 0.30690863728523254\n",
      "epoch: 195, loss was: 0.3066025972366333\n",
      "epoch: 196, loss was: 0.3062993586063385\n",
      "epoch: 197, loss was: 0.3059985637664795\n",
      "epoch: 198, loss was: 0.30570048093795776\n",
      "epoch: 199, loss was: 0.30540522933006287\n",
      "epoch: 200, loss was: 0.30511274933815\n",
      "epoch: 201, loss was: 0.3048229515552521\n",
      "epoch: 202, loss was: 0.3045358657836914\n",
      "epoch: 203, loss was: 0.30425146222114563\n",
      "epoch: 204, loss was: 0.3039696216583252\n",
      "epoch: 205, loss was: 0.30369022488594055\n",
      "epoch: 206, loss was: 0.3034134805202484\n",
      "epoch: 207, loss was: 0.30313917994499207\n",
      "epoch: 208, loss was: 0.302867591381073\n",
      "epoch: 209, loss was: 0.3025984764099121\n",
      "epoch: 210, loss was: 0.3023325204849243\n",
      "epoch: 211, loss was: 0.30206993222236633\n",
      "epoch: 212, loss was: 0.30180954933166504\n",
      "epoch: 213, loss was: 0.30155155062675476\n",
      "epoch: 214, loss was: 0.3012959361076355\n",
      "epoch: 215, loss was: 0.3010420501232147\n",
      "epoch: 216, loss was: 0.3007905185222626\n",
      "epoch: 217, loss was: 0.3005410432815552\n",
      "epoch: 218, loss was: 0.30029305815696716\n",
      "epoch: 219, loss was: 0.3000471293926239\n",
      "epoch: 220, loss was: 0.2998034656047821\n",
      "epoch: 221, loss was: 0.2995619773864746\n",
      "epoch: 222, loss was: 0.29932284355163574\n",
      "epoch: 223, loss was: 0.2990860044956207\n",
      "epoch: 224, loss was: 0.29884976148605347\n",
      "epoch: 225, loss was: 0.29861336946487427\n",
      "epoch: 226, loss was: 0.298379123210907\n",
      "epoch: 227, loss was: 0.29814720153808594\n",
      "epoch: 228, loss was: 0.2979171574115753\n",
      "epoch: 229, loss was: 0.297689288854599\n",
      "epoch: 230, loss was: 0.2974637746810913\n",
      "epoch: 231, loss was: 0.29724034667015076\n",
      "epoch: 232, loss was: 0.29701900482177734\n",
      "epoch: 233, loss was: 0.2967996597290039\n",
      "epoch: 234, loss was: 0.29658228158950806\n",
      "epoch: 235, loss was: 0.2963648736476898\n",
      "epoch: 236, loss was: 0.29614871740341187\n",
      "epoch: 237, loss was: 0.29593437910079956\n",
      "epoch: 238, loss was: 0.29572197794914246\n",
      "epoch: 239, loss was: 0.29551130533218384\n",
      "epoch: 240, loss was: 0.29530251026153564\n",
      "epoch: 241, loss was: 0.2950955033302307\n",
      "epoch: 242, loss was: 0.29489001631736755\n",
      "epoch: 243, loss was: 0.29468420147895813\n",
      "epoch: 244, loss was: 0.29448020458221436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 245, loss was: 0.29427799582481384\n",
      "epoch: 246, loss was: 0.29407748579978943\n",
      "epoch: 247, loss was: 0.2938784658908844\n",
      "epoch: 248, loss was: 0.29368114471435547\n",
      "epoch: 249, loss was: 0.29348570108413696\n",
      "epoch: 250, loss was: 0.2932918667793274\n",
      "epoch: 251, loss was: 0.2930999994277954\n",
      "epoch: 252, loss was: 0.2929101288318634\n",
      "epoch: 253, loss was: 0.2927214801311493\n",
      "epoch: 254, loss was: 0.29253461956977844\n",
      "epoch: 255, loss was: 0.2923491597175598\n",
      "epoch: 256, loss was: 0.2921653389930725\n",
      "epoch: 257, loss was: 0.2919827103614807\n",
      "epoch: 258, loss was: 0.29180192947387695\n",
      "epoch: 259, loss was: 0.2916226089000702\n",
      "epoch: 260, loss was: 0.29144465923309326\n",
      "epoch: 261, loss was: 0.2912682890892029\n",
      "epoch: 262, loss was: 0.2910933792591095\n",
      "epoch: 263, loss was: 0.2909198999404907\n",
      "epoch: 264, loss was: 0.29074785113334656\n",
      "epoch: 265, loss was: 0.2905772626399994\n",
      "epoch: 266, loss was: 0.29040807485580444\n",
      "epoch: 267, loss was: 0.2902400493621826\n",
      "epoch: 268, loss was: 0.29007330536842346\n",
      "epoch: 269, loss was: 0.28990793228149414\n",
      "epoch: 270, loss was: 0.2897438704967499\n",
      "epoch: 271, loss was: 0.28958114981651306\n",
      "epoch: 272, loss was: 0.289419561624527\n",
      "epoch: 273, loss was: 0.2892591953277588\n",
      "epoch: 274, loss was: 0.28909996151924133\n",
      "epoch: 275, loss was: 0.2889418303966522\n",
      "epoch: 276, loss was: 0.28878477215766907\n",
      "epoch: 277, loss was: 0.28862911462783813\n",
      "epoch: 278, loss was: 0.288474440574646\n",
      "epoch: 279, loss was: 0.28832098841667175\n",
      "epoch: 280, loss was: 0.28816866874694824\n",
      "epoch: 281, loss was: 0.28801751136779785\n",
      "epoch: 282, loss was: 0.2878674864768982\n",
      "epoch: 283, loss was: 0.2877185344696045\n",
      "epoch: 284, loss was: 0.28757041692733765\n",
      "epoch: 285, loss was: 0.28742343187332153\n",
      "epoch: 286, loss was: 0.287277489900589\n",
      "epoch: 287, loss was: 0.2871326208114624\n",
      "epoch: 288, loss was: 0.2869887053966522\n",
      "epoch: 289, loss was: 0.28684577345848083\n",
      "epoch: 290, loss was: 0.28670379519462585\n",
      "epoch: 291, loss was: 0.28656280040740967\n",
      "epoch: 292, loss was: 0.2864229679107666\n",
      "epoch: 293, loss was: 0.2862837612628937\n",
      "epoch: 294, loss was: 0.2861453592777252\n",
      "epoch: 295, loss was: 0.28600776195526123\n",
      "epoch: 296, loss was: 0.28587111830711365\n",
      "epoch: 297, loss was: 0.28573548793792725\n",
      "epoch: 298, loss was: 0.2856007516384125\n",
      "epoch: 299, loss was: 0.2854669392108917\n",
      "epoch: 300, loss was: 0.28533414006233215\n",
      "epoch: 301, loss was: 0.2852015197277069\n",
      "epoch: 302, loss was: 0.285068154335022\n",
      "epoch: 303, loss was: 0.28493577241897583\n",
      "epoch: 304, loss was: 0.284804105758667\n",
      "epoch: 305, loss was: 0.28467321395874023\n",
      "epoch: 306, loss was: 0.28454354405403137\n",
      "epoch: 307, loss was: 0.2844145894050598\n",
      "epoch: 308, loss was: 0.2842864990234375\n",
      "epoch: 309, loss was: 0.2841593027114868\n",
      "epoch: 310, loss was: 0.2840328514575958\n",
      "epoch: 311, loss was: 0.2839071750640869\n",
      "epoch: 312, loss was: 0.28378233313560486\n",
      "epoch: 313, loss was: 0.28365829586982727\n",
      "epoch: 314, loss was: 0.2835354506969452\n",
      "epoch: 315, loss was: 0.2834134101867676\n",
      "epoch: 316, loss was: 0.2832922339439392\n",
      "epoch: 317, loss was: 0.2831716537475586\n",
      "epoch: 318, loss was: 0.28305158019065857\n",
      "epoch: 319, loss was: 0.2829324007034302\n",
      "epoch: 320, loss was: 0.28281378746032715\n",
      "epoch: 321, loss was: 0.2826961874961853\n",
      "epoch: 322, loss was: 0.2825792729854584\n",
      "epoch: 323, loss was: 0.2824629247188568\n",
      "epoch: 324, loss was: 0.282347172498703\n",
      "epoch: 325, loss was: 0.28223201632499695\n",
      "epoch: 326, loss was: 0.2821175754070282\n",
      "epoch: 327, loss was: 0.28200361132621765\n",
      "epoch: 328, loss was: 0.28189003467559814\n",
      "epoch: 329, loss was: 0.28177690505981445\n",
      "epoch: 330, loss was: 0.2816643714904785\n",
      "epoch: 331, loss was: 0.2815525531768799\n",
      "epoch: 332, loss was: 0.28144127130508423\n",
      "epoch: 333, loss was: 0.2813306450843811\n",
      "epoch: 334, loss was: 0.2812207341194153\n",
      "epoch: 335, loss was: 0.28111207485198975\n",
      "epoch: 336, loss was: 0.28100380301475525\n",
      "epoch: 337, loss was: 0.2808961868286133\n",
      "epoch: 338, loss was: 0.2807888090610504\n",
      "epoch: 339, loss was: 0.2806820869445801\n",
      "epoch: 340, loss was: 0.2805757522583008\n",
      "epoch: 341, loss was: 0.2804699242115021\n",
      "epoch: 342, loss was: 0.28036460280418396\n",
      "epoch: 343, loss was: 0.2802598774433136\n",
      "epoch: 344, loss was: 0.2801557183265686\n",
      "epoch: 345, loss was: 0.28005215525627136\n",
      "epoch: 346, loss was: 0.2799490690231323\n",
      "epoch: 347, loss was: 0.27984651923179626\n",
      "epoch: 348, loss was: 0.27974388003349304\n",
      "epoch: 349, loss was: 0.2796412408351898\n",
      "epoch: 350, loss was: 0.27953898906707764\n",
      "epoch: 351, loss was: 0.27943697571754456\n",
      "epoch: 352, loss was: 0.27933424711227417\n",
      "epoch: 353, loss was: 0.279231995344162\n",
      "epoch: 354, loss was: 0.27913013100624084\n",
      "epoch: 355, loss was: 0.27902859449386597\n",
      "epoch: 356, loss was: 0.27892693877220154\n",
      "epoch: 357, loss was: 0.2788264751434326\n",
      "epoch: 358, loss was: 0.2787264883518219\n",
      "epoch: 359, loss was: 0.278626948595047\n",
      "epoch: 360, loss was: 0.2785278558731079\n",
      "epoch: 361, loss was: 0.27842971682548523\n",
      "epoch: 362, loss was: 0.27833202481269836\n",
      "epoch: 363, loss was: 0.2782348096370697\n",
      "epoch: 364, loss was: 0.27813804149627686\n",
      "epoch: 365, loss was: 0.27804169058799744\n",
      "epoch: 366, loss was: 0.27794620394706726\n",
      "epoch: 367, loss was: 0.27785372734069824\n",
      "epoch: 368, loss was: 0.277761846780777\n",
      "epoch: 369, loss was: 0.27767041325569153\n",
      "epoch: 370, loss was: 0.2775793671607971\n",
      "epoch: 371, loss was: 0.27748867869377136\n",
      "epoch: 372, loss was: 0.2773984670639038\n",
      "epoch: 373, loss was: 0.2773086130619049\n",
      "epoch: 374, loss was: 0.27721932530403137\n",
      "epoch: 375, loss was: 0.2771303653717041\n",
      "epoch: 376, loss was: 0.27704188227653503\n",
      "epoch: 377, loss was: 0.27695372700691223\n",
      "epoch: 378, loss was: 0.27686601877212524\n",
      "epoch: 379, loss was: 0.27677857875823975\n",
      "epoch: 380, loss was: 0.2766917645931244\n",
      "epoch: 381, loss was: 0.27660518884658813\n",
      "epoch: 382, loss was: 0.2765190005302429\n",
      "epoch: 383, loss was: 0.2764334976673126\n",
      "epoch: 384, loss was: 0.2763483226299286\n",
      "epoch: 385, loss was: 0.2762634754180908\n",
      "epoch: 386, loss was: 0.2761790156364441\n",
      "epoch: 387, loss was: 0.27609458565711975\n",
      "epoch: 388, loss was: 0.2760104537010193\n",
      "epoch: 389, loss was: 0.27592629194259644\n",
      "epoch: 390, loss was: 0.2758425176143646\n",
      "epoch: 391, loss was: 0.27575910091400146\n",
      "epoch: 392, loss was: 0.2756759226322174\n",
      "epoch: 393, loss was: 0.27559298276901245\n",
      "epoch: 394, loss was: 0.2755102813243866\n",
      "epoch: 395, loss was: 0.27542799711227417\n",
      "epoch: 396, loss was: 0.2753461003303528\n",
      "epoch: 397, loss was: 0.27526459097862244\n",
      "epoch: 398, loss was: 0.2751828730106354\n",
      "epoch: 399, loss was: 0.27510130405426025\n",
      "epoch: 400, loss was: 0.2750202715396881\n",
      "epoch: 401, loss was: 0.27493971586227417\n",
      "epoch: 402, loss was: 0.2748594284057617\n",
      "epoch: 403, loss was: 0.27477970719337463\n",
      "epoch: 404, loss was: 0.2747005522251129\n",
      "epoch: 405, loss was: 0.27462083101272583\n",
      "epoch: 406, loss was: 0.2745397686958313\n",
      "epoch: 407, loss was: 0.27445921301841736\n",
      "epoch: 408, loss was: 0.2743788957595825\n",
      "epoch: 409, loss was: 0.27429893612861633\n",
      "epoch: 410, loss was: 0.27421942353248596\n",
      "epoch: 411, loss was: 0.27414023876190186\n",
      "epoch: 412, loss was: 0.2740614712238312\n",
      "epoch: 413, loss was: 0.2739827334880829\n",
      "epoch: 414, loss was: 0.27390334010124207\n",
      "epoch: 415, loss was: 0.2738237977027893\n",
      "epoch: 416, loss was: 0.2737441658973694\n",
      "epoch: 417, loss was: 0.27366432547569275\n",
      "epoch: 418, loss was: 0.27358484268188477\n",
      "epoch: 419, loss was: 0.27350565791130066\n",
      "epoch: 420, loss was: 0.2734265923500061\n",
      "epoch: 421, loss was: 0.2733479142189026\n",
      "epoch: 422, loss was: 0.2732694149017334\n",
      "epoch: 423, loss was: 0.27319109439849854\n",
      "epoch: 424, loss was: 0.2731129825115204\n",
      "epoch: 425, loss was: 0.2730350196361542\n",
      "epoch: 426, loss was: 0.27295729517936707\n",
      "epoch: 427, loss was: 0.27287980914115906\n",
      "epoch: 428, loss was: 0.27280208468437195\n",
      "epoch: 429, loss was: 0.2727241814136505\n",
      "epoch: 430, loss was: 0.2726464867591858\n",
      "epoch: 431, loss was: 0.2725690007209778\n",
      "epoch: 432, loss was: 0.2724916934967041\n",
      "epoch: 433, loss was: 0.2724146842956543\n",
      "epoch: 434, loss was: 0.2723378837108612\n",
      "epoch: 435, loss was: 0.27226126194000244\n",
      "epoch: 436, loss was: 0.2721845507621765\n",
      "epoch: 437, loss was: 0.27210769057273865\n",
      "epoch: 438, loss was: 0.2720310389995575\n",
      "epoch: 439, loss was: 0.2719547748565674\n",
      "epoch: 440, loss was: 0.27187812328338623\n",
      "epoch: 441, loss was: 0.27180221676826477\n",
      "epoch: 442, loss was: 0.27172645926475525\n",
      "epoch: 443, loss was: 0.2716507613658905\n",
      "epoch: 444, loss was: 0.27157527208328247\n",
      "epoch: 445, loss was: 0.27150002121925354\n",
      "epoch: 446, loss was: 0.27142494916915894\n",
      "epoch: 447, loss was: 0.27135011553764343\n",
      "epoch: 448, loss was: 0.27127569913864136\n",
      "epoch: 449, loss was: 0.27120140194892883\n",
      "epoch: 450, loss was: 0.27112728357315063\n",
      "epoch: 451, loss was: 0.271053671836853\n",
      "epoch: 452, loss was: 0.27098044753074646\n",
      "epoch: 453, loss was: 0.270907461643219\n",
      "epoch: 454, loss was: 0.2708345651626587\n",
      "epoch: 455, loss was: 0.27076199650764465\n",
      "epoch: 456, loss was: 0.2706896662712097\n",
      "epoch: 457, loss was: 0.2706175446510315\n",
      "epoch: 458, loss was: 0.2705446779727936\n",
      "epoch: 459, loss was: 0.2704717516899109\n",
      "epoch: 460, loss was: 0.27039939165115356\n",
      "epoch: 461, loss was: 0.2703273296356201\n",
      "epoch: 462, loss was: 0.27025550603866577\n",
      "epoch: 463, loss was: 0.27018389105796814\n",
      "epoch: 464, loss was: 0.27011236548423767\n",
      "epoch: 465, loss was: 0.2700410783290863\n",
      "epoch: 466, loss was: 0.26996996998786926\n",
      "epoch: 467, loss was: 0.2698986828327179\n",
      "epoch: 468, loss was: 0.2698275148868561\n",
      "epoch: 469, loss was: 0.2697558104991913\n",
      "epoch: 470, loss was: 0.2696835398674011\n",
      "epoch: 471, loss was: 0.2696114480495453\n",
      "epoch: 472, loss was: 0.269539475440979\n",
      "epoch: 473, loss was: 0.26946762204170227\n",
      "epoch: 474, loss was: 0.269396036863327\n",
      "epoch: 475, loss was: 0.26932454109191895\n",
      "epoch: 476, loss was: 0.2692531645298004\n",
      "epoch: 477, loss was: 0.2691819369792938\n",
      "epoch: 478, loss was: 0.26911094784736633\n",
      "epoch: 479, loss was: 0.26904022693634033\n",
      "epoch: 480, loss was: 0.2689695954322815\n",
      "epoch: 481, loss was: 0.2688991129398346\n",
      "epoch: 482, loss was: 0.2688286304473877\n",
      "epoch: 483, loss was: 0.26875826716423035\n",
      "epoch: 484, loss was: 0.2686879336833954\n",
      "epoch: 485, loss was: 0.26861780881881714\n",
      "epoch: 486, loss was: 0.26854777336120605\n",
      "epoch: 487, loss was: 0.2684776186943054\n",
      "epoch: 488, loss was: 0.26840728521347046\n",
      "epoch: 489, loss was: 0.2683369219303131\n",
      "epoch: 490, loss was: 0.2682666778564453\n",
      "epoch: 491, loss was: 0.26819655299186707\n",
      "epoch: 492, loss was: 0.2681266665458679\n",
      "epoch: 493, loss was: 0.2680569589138031\n",
      "epoch: 494, loss was: 0.2679878771305084\n",
      "epoch: 495, loss was: 0.2679188847541809\n",
      "epoch: 496, loss was: 0.26785022020339966\n",
      "epoch: 497, loss was: 0.2677817642688751\n",
      "epoch: 498, loss was: 0.2677132785320282\n",
      "epoch: 499, loss was: 0.26764509081840515\n",
      "epoch: 500, loss was: 0.2675768733024597\n",
      "epoch: 501, loss was: 0.267508864402771\n",
      "epoch: 502, loss was: 0.2674410343170166\n",
      "epoch: 503, loss was: 0.26737380027770996\n",
      "epoch: 504, loss was: 0.2673068940639496\n",
      "epoch: 505, loss was: 0.26724016666412354\n",
      "epoch: 506, loss was: 0.2671736776828766\n",
      "epoch: 507, loss was: 0.2671073079109192\n",
      "epoch: 508, loss was: 0.2670409381389618\n",
      "epoch: 509, loss was: 0.26697471737861633\n",
      "epoch: 510, loss was: 0.2669083774089813\n",
      "epoch: 511, loss was: 0.26684150099754333\n",
      "epoch: 512, loss was: 0.2667745351791382\n",
      "epoch: 513, loss was: 0.26670777797698975\n",
      "epoch: 514, loss was: 0.2666413486003876\n",
      "epoch: 515, loss was: 0.2665751278400421\n",
      "epoch: 516, loss was: 0.26650914549827576\n",
      "epoch: 517, loss was: 0.26644331216812134\n",
      "epoch: 518, loss was: 0.2663774788379669\n",
      "epoch: 519, loss was: 0.26631104946136475\n",
      "epoch: 520, loss was: 0.26624420285224915\n",
      "epoch: 521, loss was: 0.26617738604545593\n",
      "epoch: 522, loss was: 0.2661106288433075\n",
      "epoch: 523, loss was: 0.266044020652771\n",
      "epoch: 524, loss was: 0.26597732305526733\n",
      "epoch: 525, loss was: 0.2659107446670532\n",
      "epoch: 526, loss was: 0.2658439874649048\n",
      "epoch: 527, loss was: 0.26577749848365784\n",
      "epoch: 528, loss was: 0.26571089029312134\n",
      "epoch: 529, loss was: 0.2656441926956177\n",
      "epoch: 530, loss was: 0.2655778229236603\n",
      "epoch: 531, loss was: 0.2655114233493805\n",
      "epoch: 532, loss was: 0.2654450237751007\n",
      "epoch: 533, loss was: 0.26537877321243286\n",
      "epoch: 534, loss was: 0.26531273126602173\n",
      "epoch: 535, loss was: 0.2652468681335449\n",
      "epoch: 536, loss was: 0.2651810050010681\n",
      "epoch: 537, loss was: 0.26511529088020325\n",
      "epoch: 538, loss was: 0.2650495767593384\n",
      "epoch: 539, loss was: 0.26498448848724365\n",
      "epoch: 540, loss was: 0.2649195194244385\n",
      "epoch: 541, loss was: 0.2648546099662781\n",
      "epoch: 542, loss was: 0.2647898495197296\n",
      "epoch: 543, loss was: 0.2647249102592468\n",
      "epoch: 544, loss was: 0.26466020941734314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 545, loss was: 0.26459556818008423\n",
      "epoch: 546, loss was: 0.2645314037799835\n",
      "epoch: 547, loss was: 0.26446732878685\n",
      "epoch: 548, loss was: 0.26440325379371643\n",
      "epoch: 549, loss was: 0.2643395960330963\n",
      "epoch: 550, loss was: 0.26427584886550903\n",
      "epoch: 551, loss was: 0.2642119228839874\n",
      "epoch: 552, loss was: 0.2641480565071106\n",
      "epoch: 553, loss was: 0.26408472657203674\n",
      "epoch: 554, loss was: 0.2640236020088196\n",
      "epoch: 555, loss was: 0.26396268606185913\n",
      "epoch: 556, loss was: 0.2639019191265106\n",
      "epoch: 557, loss was: 0.2638412117958069\n",
      "epoch: 558, loss was: 0.26378053426742554\n",
      "epoch: 559, loss was: 0.2637196183204651\n",
      "epoch: 560, loss was: 0.2636588215827942\n",
      "epoch: 561, loss was: 0.2635982036590576\n",
      "epoch: 562, loss was: 0.2635377049446106\n",
      "epoch: 563, loss was: 0.26347723603248596\n",
      "epoch: 564, loss was: 0.2634163796901703\n",
      "epoch: 565, loss was: 0.2633554935455322\n",
      "epoch: 566, loss was: 0.26329460740089417\n",
      "epoch: 567, loss was: 0.26323387026786804\n",
      "epoch: 568, loss was: 0.2631731927394867\n",
      "epoch: 569, loss was: 0.26311251521110535\n",
      "epoch: 570, loss was: 0.2630520164966583\n",
      "epoch: 571, loss was: 0.2629917860031128\n",
      "epoch: 572, loss was: 0.2629316449165344\n",
      "epoch: 573, loss was: 0.2628715932369232\n",
      "epoch: 574, loss was: 0.26281189918518066\n",
      "epoch: 575, loss was: 0.2627522349357605\n",
      "epoch: 576, loss was: 0.2626926600933075\n",
      "epoch: 577, loss was: 0.26263299584388733\n",
      "epoch: 578, loss was: 0.2625734210014343\n",
      "epoch: 579, loss was: 0.2625141739845276\n",
      "epoch: 580, loss was: 0.262455016374588\n",
      "epoch: 581, loss was: 0.26239556074142456\n",
      "epoch: 582, loss was: 0.2623361051082611\n",
      "epoch: 583, loss was: 0.2622765898704529\n",
      "epoch: 584, loss was: 0.26221734285354614\n",
      "epoch: 585, loss was: 0.26215818524360657\n",
      "epoch: 586, loss was: 0.26209908723831177\n",
      "epoch: 587, loss was: 0.26204025745391846\n",
      "epoch: 588, loss was: 0.2619819939136505\n",
      "epoch: 589, loss was: 0.26192378997802734\n",
      "epoch: 590, loss was: 0.26186567544937134\n",
      "epoch: 591, loss was: 0.2618075907230377\n",
      "epoch: 592, loss was: 0.2617495357990265\n",
      "epoch: 593, loss was: 0.2616916596889496\n",
      "epoch: 594, loss was: 0.26163384318351746\n",
      "epoch: 595, loss was: 0.26157620549201965\n",
      "epoch: 596, loss was: 0.26151856780052185\n",
      "epoch: 597, loss was: 0.261461079120636\n",
      "epoch: 598, loss was: 0.2614036798477173\n",
      "epoch: 599, loss was: 0.2613464593887329\n",
      "epoch: 600, loss was: 0.26128971576690674\n",
      "epoch: 601, loss was: 0.26123303174972534\n",
      "epoch: 602, loss was: 0.2611764669418335\n",
      "epoch: 603, loss was: 0.2611199617385864\n",
      "epoch: 604, loss was: 0.2610635757446289\n",
      "epoch: 605, loss was: 0.2610071003437042\n",
      "epoch: 606, loss was: 0.2609507739543915\n",
      "epoch: 607, loss was: 0.2608945071697235\n",
      "epoch: 608, loss was: 0.2608385980129242\n",
      "epoch: 609, loss was: 0.26078376173973083\n",
      "epoch: 610, loss was: 0.26072898507118225\n",
      "epoch: 611, loss was: 0.26067426800727844\n",
      "epoch: 612, loss was: 0.2606203258037567\n",
      "epoch: 613, loss was: 0.26056647300720215\n",
      "epoch: 614, loss was: 0.2605125308036804\n",
      "epoch: 615, loss was: 0.26045867800712585\n",
      "epoch: 616, loss was: 0.2604060769081116\n",
      "epoch: 617, loss was: 0.26035353541374207\n",
      "epoch: 618, loss was: 0.2603011429309845\n",
      "epoch: 619, loss was: 0.26024869084358215\n",
      "epoch: 620, loss was: 0.26019638776779175\n",
      "epoch: 621, loss was: 0.2601441740989685\n",
      "epoch: 622, loss was: 0.2600920498371124\n",
      "epoch: 623, loss was: 0.2600400447845459\n",
      "epoch: 624, loss was: 0.25998803973197937\n",
      "epoch: 625, loss was: 0.2599360942840576\n",
      "epoch: 626, loss was: 0.2598841190338135\n",
      "epoch: 627, loss was: 0.2598315179347992\n",
      "epoch: 628, loss was: 0.25977838039398193\n",
      "epoch: 629, loss was: 0.2597252428531647\n",
      "epoch: 630, loss was: 0.2596723139286041\n",
      "epoch: 631, loss was: 0.25961947441101074\n",
      "epoch: 632, loss was: 0.2595667839050293\n",
      "epoch: 633, loss was: 0.25951412320137024\n",
      "epoch: 634, loss was: 0.25946158170700073\n",
      "epoch: 635, loss was: 0.2594091594219208\n",
      "epoch: 636, loss was: 0.2593567669391632\n",
      "epoch: 637, loss was: 0.25930431485176086\n",
      "epoch: 638, loss was: 0.2592517137527466\n",
      "epoch: 639, loss was: 0.2591992914676666\n",
      "epoch: 640, loss was: 0.25914692878723145\n",
      "epoch: 641, loss was: 0.25909456610679626\n",
      "epoch: 642, loss was: 0.25904232263565063\n",
      "epoch: 643, loss was: 0.25899016857147217\n",
      "epoch: 644, loss was: 0.2589380145072937\n",
      "epoch: 645, loss was: 0.2588857114315033\n",
      "epoch: 646, loss was: 0.25883352756500244\n",
      "epoch: 647, loss was: 0.2587813138961792\n",
      "epoch: 648, loss was: 0.2587294578552246\n",
      "epoch: 649, loss was: 0.25867825746536255\n",
      "epoch: 650, loss was: 0.25862717628479004\n",
      "epoch: 651, loss was: 0.2585761845111847\n",
      "epoch: 652, loss was: 0.2585257589817047\n",
      "epoch: 653, loss was: 0.25847527384757996\n",
      "epoch: 654, loss was: 0.25842490792274475\n",
      "epoch: 655, loss was: 0.2583746016025543\n",
      "epoch: 656, loss was: 0.25832435488700867\n",
      "epoch: 657, loss was: 0.2582744061946869\n",
      "epoch: 658, loss was: 0.25822436809539795\n",
      "epoch: 659, loss was: 0.2581743896007538\n",
      "epoch: 660, loss was: 0.25812456011772156\n",
      "epoch: 661, loss was: 0.2580748200416565\n",
      "epoch: 662, loss was: 0.2580253481864929\n",
      "epoch: 663, loss was: 0.25797560811042786\n",
      "epoch: 664, loss was: 0.25792595744132996\n",
      "epoch: 665, loss was: 0.2578762471675873\n",
      "epoch: 666, loss was: 0.25782665610313416\n",
      "epoch: 667, loss was: 0.2577769458293915\n",
      "epoch: 668, loss was: 0.2577274441719055\n",
      "epoch: 669, loss was: 0.25767782330513\n",
      "epoch: 670, loss was: 0.2576283812522888\n",
      "epoch: 671, loss was: 0.25757908821105957\n",
      "epoch: 672, loss was: 0.2575298249721527\n",
      "epoch: 673, loss was: 0.25747984647750854\n",
      "epoch: 674, loss was: 0.25742974877357483\n",
      "epoch: 675, loss was: 0.2573794722557068\n",
      "epoch: 676, loss was: 0.25732916593551636\n",
      "epoch: 677, loss was: 0.25727906823158264\n",
      "epoch: 678, loss was: 0.25722911953926086\n",
      "epoch: 679, loss was: 0.257179319858551\n",
      "epoch: 680, loss was: 0.25713011622428894\n",
      "epoch: 681, loss was: 0.25708073377609253\n",
      "epoch: 682, loss was: 0.2570313811302185\n",
      "epoch: 683, loss was: 0.2569822072982788\n",
      "epoch: 684, loss was: 0.2569330632686615\n",
      "epoch: 685, loss was: 0.25688397884368896\n",
      "epoch: 686, loss was: 0.2568349540233612\n",
      "epoch: 687, loss was: 0.256786048412323\n",
      "epoch: 688, loss was: 0.25673723220825195\n",
      "epoch: 689, loss was: 0.25668853521347046\n",
      "epoch: 690, loss was: 0.25663989782333374\n",
      "epoch: 691, loss was: 0.2565915286540985\n",
      "epoch: 692, loss was: 0.25654301047325134\n",
      "epoch: 693, loss was: 0.25649482011795044\n",
      "epoch: 694, loss was: 0.2564466893672943\n",
      "epoch: 695, loss was: 0.25639864802360535\n",
      "epoch: 696, loss was: 0.25635066628456116\n",
      "epoch: 697, loss was: 0.25630277395248413\n",
      "epoch: 698, loss was: 0.25625497102737427\n",
      "epoch: 699, loss was: 0.25620728731155396\n",
      "epoch: 700, loss was: 0.2561597526073456\n",
      "epoch: 701, loss was: 0.25611209869384766\n",
      "epoch: 702, loss was: 0.25606462359428406\n",
      "epoch: 703, loss was: 0.25601726770401\n",
      "epoch: 704, loss was: 0.2559700310230255\n",
      "epoch: 705, loss was: 0.2559228539466858\n",
      "epoch: 706, loss was: 0.25587594509124756\n",
      "epoch: 707, loss was: 0.25582924485206604\n",
      "epoch: 708, loss was: 0.2557823657989502\n",
      "epoch: 709, loss was: 0.2557358145713806\n",
      "epoch: 710, loss was: 0.2556892931461334\n",
      "epoch: 711, loss was: 0.255642831325531\n",
      "epoch: 712, loss was: 0.25559642910957336\n",
      "epoch: 713, loss was: 0.2555501461029053\n",
      "epoch: 714, loss was: 0.2555040717124939\n",
      "epoch: 715, loss was: 0.25545787811279297\n",
      "epoch: 716, loss was: 0.2554120123386383\n",
      "epoch: 717, loss was: 0.25536614656448364\n",
      "epoch: 718, loss was: 0.25532039999961853\n",
      "epoch: 719, loss was: 0.2552739679813385\n",
      "epoch: 720, loss was: 0.255227267742157\n",
      "epoch: 721, loss was: 0.25518059730529785\n",
      "epoch: 722, loss was: 0.25513404607772827\n",
      "epoch: 723, loss was: 0.2550875246524811\n",
      "epoch: 724, loss was: 0.25504106283187866\n",
      "epoch: 725, loss was: 0.2549946904182434\n",
      "epoch: 726, loss was: 0.25494834780693054\n",
      "epoch: 727, loss was: 0.25490203499794006\n",
      "epoch: 728, loss was: 0.2548559308052063\n",
      "epoch: 729, loss was: 0.2548098564147949\n",
      "epoch: 730, loss was: 0.25476452708244324\n",
      "epoch: 731, loss was: 0.2547202706336975\n",
      "epoch: 732, loss was: 0.25467628240585327\n",
      "epoch: 733, loss was: 0.2546321451663971\n",
      "epoch: 734, loss was: 0.2545880973339081\n",
      "epoch: 735, loss was: 0.25454410910606384\n",
      "epoch: 736, loss was: 0.25450003147125244\n",
      "epoch: 737, loss was: 0.2544560134410858\n",
      "epoch: 738, loss was: 0.25441208481788635\n",
      "epoch: 739, loss was: 0.25436824560165405\n",
      "epoch: 740, loss was: 0.25432443618774414\n",
      "epoch: 741, loss was: 0.2542804479598999\n",
      "epoch: 742, loss was: 0.2542365789413452\n",
      "epoch: 743, loss was: 0.2541927993297577\n",
      "epoch: 744, loss was: 0.25414904952049255\n",
      "epoch: 745, loss was: 0.2541052997112274\n",
      "epoch: 746, loss was: 0.2540615499019623\n",
      "epoch: 747, loss was: 0.25401782989501953\n",
      "epoch: 748, loss was: 0.25397416949272156\n",
      "epoch: 749, loss was: 0.25393053889274597\n",
      "epoch: 750, loss was: 0.2538870573043823\n",
      "epoch: 751, loss was: 0.2538435161113739\n",
      "epoch: 752, loss was: 0.2538001835346222\n",
      "epoch: 753, loss was: 0.2537568211555481\n",
      "epoch: 754, loss was: 0.25371360778808594\n",
      "epoch: 755, loss was: 0.25367048382759094\n",
      "epoch: 756, loss was: 0.2536274492740631\n",
      "epoch: 757, loss was: 0.2535843849182129\n",
      "epoch: 758, loss was: 0.25354140996932983\n",
      "epoch: 759, loss was: 0.2534981369972229\n",
      "epoch: 760, loss was: 0.25345486402511597\n",
      "epoch: 761, loss was: 0.25341182947158813\n",
      "epoch: 762, loss was: 0.25336897373199463\n",
      "epoch: 763, loss was: 0.2533259987831116\n",
      "epoch: 764, loss was: 0.25328329205513\n",
      "epoch: 765, loss was: 0.2532406151294708\n",
      "epoch: 766, loss was: 0.25319793820381165\n",
      "epoch: 767, loss was: 0.25315621495246887\n",
      "epoch: 768, loss was: 0.25311535596847534\n",
      "epoch: 769, loss was: 0.2530745565891266\n",
      "epoch: 770, loss was: 0.25303375720977783\n",
      "epoch: 771, loss was: 0.252993106842041\n",
      "epoch: 772, loss was: 0.25295233726501465\n",
      "epoch: 773, loss was: 0.25291165709495544\n",
      "epoch: 774, loss was: 0.25287097692489624\n",
      "epoch: 775, loss was: 0.25283023715019226\n",
      "epoch: 776, loss was: 0.2527894973754883\n",
      "epoch: 777, loss was: 0.25274884700775146\n",
      "epoch: 778, loss was: 0.2527082860469818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 779, loss was: 0.25266793370246887\n",
      "epoch: 780, loss was: 0.25262773036956787\n",
      "epoch: 781, loss was: 0.2525878846645355\n",
      "epoch: 782, loss was: 0.25254806876182556\n",
      "epoch: 783, loss was: 0.2525083124637604\n",
      "epoch: 784, loss was: 0.25246864557266235\n",
      "epoch: 785, loss was: 0.2524288594722748\n",
      "epoch: 786, loss was: 0.25238925218582153\n",
      "epoch: 787, loss was: 0.25234994292259216\n",
      "epoch: 788, loss was: 0.25231072306632996\n",
      "epoch: 789, loss was: 0.2522715926170349\n",
      "epoch: 790, loss was: 0.25223270058631897\n",
      "epoch: 791, loss was: 0.25219377875328064\n",
      "epoch: 792, loss was: 0.2521548271179199\n",
      "epoch: 793, loss was: 0.25211647152900696\n",
      "epoch: 794, loss was: 0.25207817554473877\n",
      "epoch: 795, loss was: 0.25203990936279297\n",
      "epoch: 796, loss was: 0.2520018219947815\n",
      "epoch: 797, loss was: 0.2519637942314148\n",
      "epoch: 798, loss was: 0.2519257664680481\n",
      "epoch: 799, loss was: 0.25188764929771423\n",
      "epoch: 800, loss was: 0.2518499493598938\n",
      "epoch: 801, loss was: 0.25181230902671814\n",
      "epoch: 802, loss was: 0.25177472829818726\n",
      "epoch: 803, loss was: 0.25173699855804443\n",
      "epoch: 804, loss was: 0.25169968605041504\n",
      "epoch: 805, loss was: 0.2516624331474304\n",
      "epoch: 806, loss was: 0.2516252100467682\n",
      "epoch: 807, loss was: 0.25158804655075073\n",
      "epoch: 808, loss was: 0.25155097246170044\n",
      "epoch: 809, loss was: 0.2515137493610382\n",
      "epoch: 810, loss was: 0.2514764666557312\n",
      "epoch: 811, loss was: 0.25143924355506897\n",
      "epoch: 812, loss was: 0.2514022886753082\n",
      "epoch: 813, loss was: 0.2513655424118042\n",
      "epoch: 814, loss was: 0.2513289153575897\n",
      "epoch: 815, loss was: 0.25129222869873047\n",
      "epoch: 816, loss was: 0.25125566124916077\n",
      "epoch: 817, loss was: 0.25121915340423584\n",
      "epoch: 818, loss was: 0.2511826455593109\n",
      "epoch: 819, loss was: 0.25114622712135315\n",
      "epoch: 820, loss was: 0.25110989809036255\n",
      "epoch: 821, loss was: 0.25107356905937195\n",
      "epoch: 822, loss was: 0.2510373294353485\n",
      "epoch: 823, loss was: 0.2510010898113251\n",
      "epoch: 824, loss was: 0.25096502900123596\n",
      "epoch: 825, loss was: 0.25092920660972595\n",
      "epoch: 826, loss was: 0.25089359283447266\n",
      "epoch: 827, loss was: 0.2508580684661865\n",
      "epoch: 828, loss was: 0.25082263350486755\n",
      "epoch: 829, loss was: 0.25078728795051575\n",
      "epoch: 830, loss was: 0.25075197219848633\n",
      "epoch: 831, loss was: 0.2507166862487793\n",
      "epoch: 832, loss was: 0.25068148970603943\n",
      "epoch: 833, loss was: 0.25064617395401\n",
      "epoch: 834, loss was: 0.2506110668182373\n",
      "epoch: 835, loss was: 0.25057584047317505\n",
      "epoch: 836, loss was: 0.2505406141281128\n",
      "epoch: 837, loss was: 0.2505054473876953\n",
      "epoch: 838, loss was: 0.25047048926353455\n",
      "epoch: 839, loss was: 0.2504355013370514\n",
      "epoch: 840, loss was: 0.2504004240036011\n",
      "epoch: 841, loss was: 0.25036531686782837\n",
      "epoch: 842, loss was: 0.25033047795295715\n",
      "epoch: 843, loss was: 0.25029560923576355\n",
      "epoch: 844, loss was: 0.25026071071624756\n",
      "epoch: 845, loss was: 0.25022584199905396\n",
      "epoch: 846, loss was: 0.25019019842147827\n",
      "epoch: 847, loss was: 0.2501542568206787\n",
      "epoch: 848, loss was: 0.2501183748245239\n",
      "epoch: 849, loss was: 0.2500825524330139\n",
      "epoch: 850, loss was: 0.2500467598438263\n",
      "epoch: 851, loss was: 0.2500108480453491\n",
      "epoch: 852, loss was: 0.2499750405550003\n",
      "epoch: 853, loss was: 0.24993930757045746\n",
      "epoch: 854, loss was: 0.2499035745859146\n",
      "epoch: 855, loss was: 0.24986791610717773\n",
      "epoch: 856, loss was: 0.24983230233192444\n",
      "epoch: 857, loss was: 0.24979674816131592\n",
      "epoch: 858, loss was: 0.2497612088918686\n",
      "epoch: 859, loss was: 0.24972572922706604\n",
      "epoch: 860, loss was: 0.24969035387039185\n",
      "epoch: 861, loss was: 0.24965499341487885\n",
      "epoch: 862, loss was: 0.24962010979652405\n",
      "epoch: 863, loss was: 0.24958528578281403\n",
      "epoch: 864, loss was: 0.2495504915714264\n",
      "epoch: 865, loss was: 0.24951569736003876\n",
      "epoch: 866, loss was: 0.24948091804981232\n",
      "epoch: 867, loss was: 0.24944622814655304\n",
      "epoch: 868, loss was: 0.24941155314445496\n",
      "epoch: 869, loss was: 0.24937692284584045\n",
      "epoch: 870, loss was: 0.24934235215187073\n",
      "epoch: 871, loss was: 0.24930782616138458\n",
      "epoch: 872, loss was: 0.24927332997322083\n",
      "epoch: 873, loss was: 0.24923883378505707\n",
      "epoch: 874, loss was: 0.24920444190502167\n",
      "epoch: 875, loss was: 0.24917028844356537\n",
      "epoch: 876, loss was: 0.2491360902786255\n",
      "epoch: 877, loss was: 0.24910195171833038\n",
      "epoch: 878, loss was: 0.24906782805919647\n",
      "epoch: 879, loss was: 0.24903377890586853\n",
      "epoch: 880, loss was: 0.24899974465370178\n",
      "epoch: 881, loss was: 0.248965784907341\n",
      "epoch: 882, loss was: 0.2489311695098877\n",
      "epoch: 883, loss was: 0.24889659881591797\n",
      "epoch: 884, loss was: 0.24886222183704376\n",
      "epoch: 885, loss was: 0.2488277107477188\n",
      "epoch: 886, loss was: 0.24879305064678192\n",
      "epoch: 887, loss was: 0.2487584799528122\n",
      "epoch: 888, loss was: 0.24872387945652008\n",
      "epoch: 889, loss was: 0.24868902564048767\n",
      "epoch: 890, loss was: 0.2486545443534851\n",
      "epoch: 891, loss was: 0.24862006306648254\n",
      "epoch: 892, loss was: 0.24858562648296356\n",
      "epoch: 893, loss was: 0.24855121970176697\n",
      "epoch: 894, loss was: 0.24851685762405396\n",
      "epoch: 895, loss was: 0.24848255515098572\n",
      "epoch: 896, loss was: 0.2484479695558548\n",
      "epoch: 897, loss was: 0.24841371178627014\n",
      "epoch: 898, loss was: 0.2483794242143631\n",
      "epoch: 899, loss was: 0.24834512174129486\n",
      "epoch: 900, loss was: 0.24831083416938782\n",
      "epoch: 901, loss was: 0.24827660620212555\n",
      "epoch: 902, loss was: 0.2482423484325409\n",
      "epoch: 903, loss was: 0.24820812046527863\n",
      "epoch: 904, loss was: 0.24817399680614471\n",
      "epoch: 905, loss was: 0.24813999235630035\n",
      "epoch: 906, loss was: 0.24810580909252167\n",
      "epoch: 907, loss was: 0.24807193875312805\n",
      "epoch: 908, loss was: 0.24803811311721802\n",
      "epoch: 909, loss was: 0.24800433218479156\n",
      "epoch: 910, loss was: 0.2479705661535263\n",
      "epoch: 911, loss was: 0.24793611466884613\n",
      "epoch: 912, loss was: 0.2479017674922943\n",
      "epoch: 913, loss was: 0.2478674352169037\n",
      "epoch: 914, loss was: 0.24783311784267426\n",
      "epoch: 915, loss was: 0.2477988749742508\n",
      "epoch: 916, loss was: 0.24776466190814972\n",
      "epoch: 917, loss was: 0.24773064255714417\n",
      "epoch: 918, loss was: 0.24769672751426697\n",
      "epoch: 919, loss was: 0.24766284227371216\n",
      "epoch: 920, loss was: 0.24762900173664093\n",
      "epoch: 921, loss was: 0.2475951761007309\n",
      "epoch: 922, loss was: 0.24756130576133728\n",
      "epoch: 923, loss was: 0.24752762913703918\n",
      "epoch: 924, loss was: 0.24749396741390228\n",
      "epoch: 925, loss was: 0.24746035039424896\n",
      "epoch: 926, loss was: 0.24742674827575684\n",
      "epoch: 927, loss was: 0.2473931759595871\n",
      "epoch: 928, loss was: 0.24735935032367706\n",
      "epoch: 929, loss was: 0.24732552468776703\n",
      "epoch: 930, loss was: 0.247291699051857\n",
      "epoch: 931, loss was: 0.24725794792175293\n",
      "epoch: 932, loss was: 0.24722398817539215\n",
      "epoch: 933, loss was: 0.2471899837255478\n",
      "epoch: 934, loss was: 0.2471560835838318\n",
      "epoch: 935, loss was: 0.24712207913398743\n",
      "epoch: 936, loss was: 0.247088223695755\n",
      "epoch: 937, loss was: 0.24705439805984497\n",
      "epoch: 938, loss was: 0.24702002108097076\n",
      "epoch: 939, loss was: 0.24698518216609955\n",
      "epoch: 940, loss was: 0.24695047736167908\n",
      "epoch: 941, loss was: 0.2469155490398407\n",
      "epoch: 942, loss was: 0.24687987565994263\n",
      "epoch: 943, loss was: 0.24684424698352814\n",
      "epoch: 944, loss was: 0.24680866301059723\n",
      "epoch: 945, loss was: 0.2467731237411499\n",
      "epoch: 946, loss was: 0.24673764407634735\n",
      "epoch: 947, loss was: 0.24670222401618958\n",
      "epoch: 948, loss was: 0.246666818857193\n",
      "epoch: 949, loss was: 0.24663148820400238\n",
      "epoch: 950, loss was: 0.24659615755081177\n",
      "epoch: 951, loss was: 0.24656087160110474\n",
      "epoch: 952, loss was: 0.24652519822120667\n",
      "epoch: 953, loss was: 0.2464895248413086\n",
      "epoch: 954, loss was: 0.2464539110660553\n",
      "epoch: 955, loss was: 0.24641834199428558\n",
      "epoch: 956, loss was: 0.24638275802135468\n",
      "epoch: 957, loss was: 0.24634777009487152\n",
      "epoch: 958, loss was: 0.24631276726722717\n",
      "epoch: 959, loss was: 0.24627771973609924\n",
      "epoch: 960, loss was: 0.24624283611774445\n",
      "epoch: 961, loss was: 0.24620798230171204\n",
      "epoch: 962, loss was: 0.24617312848567963\n",
      "epoch: 963, loss was: 0.24613843858242035\n",
      "epoch: 964, loss was: 0.24610373377799988\n",
      "epoch: 965, loss was: 0.2460688203573227\n",
      "epoch: 966, loss was: 0.24603408575057983\n",
      "epoch: 967, loss was: 0.24599944055080414\n",
      "epoch: 968, loss was: 0.24596476554870605\n",
      "epoch: 969, loss was: 0.24593010544776917\n",
      "epoch: 970, loss was: 0.2458953857421875\n",
      "epoch: 971, loss was: 0.2458607703447342\n",
      "epoch: 972, loss was: 0.24582621455192566\n",
      "epoch: 973, loss was: 0.24579156935214996\n",
      "epoch: 974, loss was: 0.24575696885585785\n",
      "epoch: 975, loss was: 0.24572235345840454\n",
      "epoch: 976, loss was: 0.245687797665596\n",
      "epoch: 977, loss was: 0.24565324187278748\n",
      "epoch: 978, loss was: 0.24561870098114014\n",
      "epoch: 979, loss was: 0.24558430910110474\n",
      "epoch: 980, loss was: 0.24554991722106934\n",
      "epoch: 981, loss was: 0.24551557004451752\n",
      "epoch: 982, loss was: 0.24548125267028809\n",
      "epoch: 983, loss was: 0.24544702470302582\n",
      "epoch: 984, loss was: 0.24541281163692474\n",
      "epoch: 985, loss was: 0.24537861347198486\n",
      "epoch: 986, loss was: 0.24534441530704498\n",
      "epoch: 987, loss was: 0.24531015753746033\n",
      "epoch: 988, loss was: 0.24527589976787567\n",
      "epoch: 989, loss was: 0.24524171650409698\n",
      "epoch: 990, loss was: 0.2452075332403183\n",
      "epoch: 991, loss was: 0.2451733946800232\n",
      "epoch: 992, loss was: 0.2451389878988266\n",
      "epoch: 993, loss was: 0.2451046109199524\n",
      "epoch: 994, loss was: 0.24507032334804535\n",
      "epoch: 995, loss was: 0.24503600597381592\n",
      "epoch: 996, loss was: 0.24500171840190887\n",
      "epoch: 997, loss was: 0.2449674904346466\n",
      "epoch: 998, loss was: 0.2449333816766739\n",
      "epoch: 999, loss was: 0.24489931762218475\n",
      "epoch: 1000, loss was: 0.24486549198627472\n"
     ]
    }
   ],
   "source": [
    "#set number of epochs\n",
    "n_epochs = 1000\n",
    "#set number of batches of data points\n",
    "batch_s = 100\n",
    "\n",
    "#train model\n",
    "for epoch in range(n_epochs):\n",
    "    neural_network.train()\n",
    "    for i in range (0, len(X_train_tensor), batch_s):\n",
    "        #separate x, y into batches, make preds, and calculate loss\n",
    "        X_batch = X_train_tensor[i:i+batch_s]\n",
    "        y_batch = y_train_tensor[i:i+batch_s]\n",
    "        \n",
    "        y_pred = neural_network(X_batch)\n",
    "        \n",
    "        loss = loss_criterion(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {epoch+1}, loss was: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb2dba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.4478584825992584\n"
     ]
    }
   ],
   "source": [
    "#first round after epoch =100\n",
    "#evaluate model\n",
    "neural_network.eval()\n",
    "#compare probability predictions to y_test values\n",
    "with torch.no_grad():\n",
    "    test_out = neural_network(X_test_tensor)\n",
    "    test_loss = loss_criterion(test_out, y_test_tensor)\n",
    "\n",
    "print(f'test loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef0f424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2420000433921814\n"
     ]
    }
   ],
   "source": [
    "#after changing epoch = 1000\n",
    "#evaluate model\n",
    "neural_network.eval()\n",
    "#compare probability predictions to y_test values\n",
    "with torch.no_grad():\n",
    "    test_output = neural_network(X_test_tensor)\n",
    "    test_loss = loss_criterion(test_output, y_test_tensor)\n",
    "\n",
    "print(f'test loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f6edbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8961)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate accuracy of second model\n",
    "accuracy = (test_output.round() == y_test_tensor).float().mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27226e11",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798e5e6",
   "metadata": {},
   "source": [
    "Initially, I had mislabeled the input and output variables in the data, where the outputs were already in binary format. Due to my standardization of the data points in the wrong format, the loss values of the neural network were coming out negative, which is unusual and indicates an error within the data processing or build of the model itself since the values should range between 0 and 1 when applying the binary cross entropy loss function I did due to the outcome of predicting the (multiclass) probabilities of having a specific fault in the steel plate. \n",
    "\n",
    "After fixing the variable assignment, and maintaining the standardization of the numerical independent features, the BCELoss outputs ranged within the expected 0-1 values. I initally ran my model with 100 epochs and while the epoch and batch iteration the model's continuous improvement became evident, where the training loss decreased to 0.45, down from 0.7 in the first epoch, (the test loss was also 0.45). Although this loss value indicated the model performed better than random guessing/predicting, I decided to run the code again with 1,000 epochs to compare the improvement after many more iterations of learning. The training of the model improved significantly, where the last loss output was 0.2448, and then the test loss came out to  0.2420, which was slightly better than the last training round. \n",
    "\n",
    "Since the loss value can be a bit hard to interpret, I computed the accuracy as well, by comparing the output to the original label and found the model to have an accuracy of about 90%, which is an impressive outcome considering no extreneous fine-tuning was done up to this point, and considering the 1,000 epochs didn't seem to present too much of a computational problem, I could imagine the neural network model achieving close to 100% accuracy with more training/learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
