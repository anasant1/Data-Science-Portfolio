{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd8d843",
   "metadata": {},
   "source": [
    "# 6.6 Assignment 6: Naïve Bayes\n",
    "\n",
    "### TABLE OF CONTENTS\n",
    "[Baye's Theorem](#Baye's-Theorem)\n",
    "\n",
    "[YouTube Spam Comments: NB application](#YouTube-Spam-Comments:-NB-application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10122a",
   "metadata": {},
   "source": [
    "# Baye's Theorem\n",
    "\n",
    "This section explains how to turn P(E|H) to P(H|E), with E=Evidence and H=Hypothesis in layman's terms, and utilizes a real-life example to demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdaa03b",
   "metadata": {},
   "source": [
    "Conditional probability makes  P(E|H) = P(H|E) because of its property of symmetry. This means that the probability of seeing an observation/evidence given a true hypothesis P(E|H) is the same as the probability of the hypothesis being true given the evidence/observation. Essentially, it's asking a very similar question just with a different focus, P(H|E) asks what the probability is of the hypothesis being true with the given evidence, and P(E|H) asks what the probability of seeing the evidence with the true hypothesis. \n",
    "\n",
    "For example, in a scenario of spam email classification:\n",
    "Let's say E is a given word in an email (like \"promotion\"), and H is our hypothesis that the email is indeed spam.\n",
    "\n",
    "We could ask P(E|H): what is the probability the email contains the word \"promotion\" (E) given the email is spam (H)? This is the same as asking P(H|E): what is the probability that the email is spam given that it contains the word \"promotion\" in it? Both questions essentially ask the same question, but focus on a different component of the algorithm -one deals directly with the probability of the evidence specifically while the other chooses to focus on the probability of the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e168466",
   "metadata": {},
   "source": [
    "# YouTube Spam Comments: NB application\n",
    "\n",
    "This section utilizes 5 datasets, 4 for training, and the fifth for testing of the application of the NB algorithm.The datasets are composed of 1,956 real messages extracted from five videos. These five videos are popular pop songs that were among the 10 most viewed in the collection period. \n",
    "\n",
    "#### Dataset Attributes:\n",
    "- COMMENT_ID: Unique ID representing the comment\n",
    "\n",
    "- AUTHOR: Author ID\n",
    "\n",
    "- DATE: Date the comment is posted\n",
    "\n",
    "- CONTENT: The comment\n",
    "\n",
    "- TAG: Attribute Explained\n",
    "\n",
    "\n",
    "#### GOALS:\n",
    "For this exercise use any four of these five datasets to build a spam filter with the Naïve Bayes approach. \n",
    "\n",
    "Use that filter to check the accuracy of the remaining dataset.\n",
    "\n",
    "Make sure to report the details of your training and the model.\n",
    "\n",
    "### TABLE OF CONTENTS\n",
    "[Libraries](#Libraries)\n",
    "\n",
    "[Data Preprocessing](#Data-Preprocessing)\n",
    "\n",
    "[NB Model Training](#NB-Model-Training)\n",
    "\n",
    "[Model Testing](#Model-Testing)\n",
    "\n",
    "[Overall Conclusions](#Overall-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced991fc",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcb75ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anasantiago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f3b10d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anasantiago/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8bd91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d93e4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92db976",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdcefe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load first dataset\n",
    "psy_data = pd.read_csv('Youtube01-Psy.csv')\n",
    "psy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de285245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary cols\n",
    "psy_clean = psy_data.copy()\n",
    "psy_clean = psy_clean.drop(columns=['COMMENT_ID', 'AUTHOR', 'DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7056c5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0    175\n",
       "1    175\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine data distribution by class\n",
    "psy_clean.groupby(by = ['CLASS']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da28cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z12pgdhovmrktzm3i23es5d5junftft3f</td>\n",
       "      <td>lekanaVEVO1</td>\n",
       "      <td>2014-07-22T15:27:50</td>\n",
       "      <td>i love this so much. AND also I Generate Free ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z13yx345uxepetggz04ci5rjcxeohzlrtf4</td>\n",
       "      <td>Pyunghee</td>\n",
       "      <td>2014-07-27T01:57:16</td>\n",
       "      <td>http://www.billboard.com/articles/columns/pop-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z12lsjvi3wa5x1vwh04cibeaqnzrevxajw00k</td>\n",
       "      <td>Erica Ross</td>\n",
       "      <td>2014-07-27T02:51:43</td>\n",
       "      <td>Hey guys! Please join me in my fight to help a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jcjuovxbwfr0ge04cev2ipsjdfdurwck</td>\n",
       "      <td>Aviel Haimov</td>\n",
       "      <td>2014-08-01T12:27:48</td>\n",
       "      <td>http://psnboss.com/?ref=2tGgp3pV6L this is the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13qybua2yfydzxzj04cgfpqdt2syfx53ms0k</td>\n",
       "      <td>John Bello</td>\n",
       "      <td>2014-08-01T21:04:03</td>\n",
       "      <td>Hey everyone. Watch this trailer!!!!!!!!  http...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              COMMENT_ID        AUTHOR                 DATE  \\\n",
       "0      z12pgdhovmrktzm3i23es5d5junftft3f   lekanaVEVO1  2014-07-22T15:27:50   \n",
       "1    z13yx345uxepetggz04ci5rjcxeohzlrtf4      Pyunghee  2014-07-27T01:57:16   \n",
       "2  z12lsjvi3wa5x1vwh04cibeaqnzrevxajw00k    Erica Ross  2014-07-27T02:51:43   \n",
       "3    z13jcjuovxbwfr0ge04cev2ipsjdfdurwck  Aviel Haimov  2014-08-01T12:27:48   \n",
       "4  z13qybua2yfydzxzj04cgfpqdt2syfx53ms0k    John Bello  2014-08-01T21:04:03   \n",
       "\n",
       "                                             CONTENT  CLASS  \n",
       "0  i love this so much. AND also I Generate Free ...      1  \n",
       "1  http://www.billboard.com/articles/columns/pop-...      1  \n",
       "2  Hey guys! Please join me in my fight to help a...      1  \n",
       "3  http://psnboss.com/?ref=2tGgp3pV6L this is the...      1  \n",
       "4  Hey everyone. Watch this trailer!!!!!!!!  http...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load next dataset\n",
    "kp_data = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "kp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d472a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0    175\n",
       "1    175\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean data & examine distribution by class\n",
    "kp_clean = kp_data.copy()\n",
    "#drop unnecessary cols\n",
    "kp_clean = kp_clean.drop(columns=['COMMENT_ID', 'AUTHOR', 'DATE'])\n",
    "kp_clean.groupby(by=['CLASS']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9acd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0    202\n",
       "1    236\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load third df\n",
    "lmfao_data = pd.read_csv('Youtube03-LMFAO.csv')\n",
    "lmfao_clean = lmfao_data.copy()\n",
    "#drop unnecessary cols\n",
    "lmfao_clean = lmfao_clean.drop(columns=['COMMENT_ID', 'AUTHOR', 'DATE'])\n",
    "#examine distributions\n",
    "lmfao_clean.groupby(by=['CLASS']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd135ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0    203\n",
       "1    245\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load fourth df\n",
    "eminem_data = pd.read_csv('Youtube04-Eminem.csv')\n",
    "eminem_clean = eminem_data.copy()\n",
    "#drop unnecessary cols\n",
    "eminem_clean = eminem_clean.drop(columns=['COMMENT_ID', 'AUTHOR', 'DATE'])\n",
    "#examine distributions\n",
    "eminem_clean.groupby(by=['CLASS']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea211fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS\n",
       "0    196\n",
       "1    174\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load last df\n",
    "shakira_data = pd.read_csv('Youtube05-Shakira.csv')\n",
    "shakira_clean = shakira_data.copy()\n",
    "#drop unnecessary cols\n",
    "shakira_clean = shakira_clean.drop(columns=['COMMENT_ID', 'AUTHOR', 'DATE'])\n",
    "#examine distributions\n",
    "shakira_clean.groupby(by=['CLASS']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f4ec5505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psy_list = psy_clean['CONTENT'].tolist()\n",
    "#psy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5368c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    check this out \\ufeff'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create function to clean up text, remove links\n",
    "sample_text = 'watch?v=vtaRGgvGtWQ   2014 Check this out .\\ufeff'\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    #remove links\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'htt\\S+', '', text)\n",
    "    #remove youtube specific links\n",
    "    text = re.sub(r'watch\\?v=\\S+', '', text)\n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "new_sample = remove_punctuation(sample_text)\n",
    "new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c268e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c09815bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: kobyoshi02</td>\n",
       "      <td>1</td>\n",
       "      <td>huh anyway check out this youtube channel kobyoshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys check out my new channel and our first vid this is us the  monkeys im the monkey in the white shirtplease leave a like comment  and please subscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>just for test i have to say murdevcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy  ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>check this out ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hey, check out my new website!! This site is about kids stuff. kidsmediausa  . com</td>\n",
       "      <td>1</td>\n",
       "      <td>hey check out my new website this site is about kids stuff kidsmediausa   com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>subscribe to my channel ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i turned it on mute as soon is i came on i just wanted to check the  views...﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>i turned it on mute as soon is i came on i just wanted to check the  views﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You should check my channel for Funny VIDEOS!!﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>you should check my channel for funny videos﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  CONTENT  \\\n",
       "0                                                                                                                Huh, anyway check out this you[tube] channel: kobyoshi02   \n",
       "1  Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!   \n",
       "2                                                                                                                                  just for test I have to say murdev.com   \n",
       "3                                                                                                                        me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                                                                                                                                 watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "5                                                                                      Hey, check out my new website!! This site is about kids stuff. kidsmediausa  . com   \n",
       "6                                                                                                                                               Subscribe to my channel ﻿   \n",
       "7                                                                                          i turned it on mute as soon is i came on i just wanted to check the  views...﻿   \n",
       "8                                                                                                                         You should check my channel for Funny VIDEOS!!﻿   \n",
       "\n",
       "   CLASS  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      1   \n",
       "3      1   \n",
       "4      1   \n",
       "5      1   \n",
       "6      1   \n",
       "7      0   \n",
       "8      1   \n",
       "\n",
       "                                                                                                                                              processed_comments  \n",
       "0                                                                                                             huh anyway check out this youtube channel kobyoshi  \n",
       "1  hey guys check out my new channel and our first vid this is us the  monkeys im the monkey in the white shirtplease leave a like comment  and please subscribe  \n",
       "2                                                                                                                          just for test i have to say murdevcom  \n",
       "3                                                                                                                  me shaking my sexy ass on my channel enjoy  ﻿  \n",
       "4                                                                                                                                               check this out ﻿  \n",
       "5                                                                                  hey check out my new website this site is about kids stuff kidsmediausa   com  \n",
       "6                                                                                                                                      subscribe to my channel ﻿  \n",
       "7                                                                                    i turned it on mute as soon is i came on i just wanted to check the  views﻿  \n",
       "8                                                                                                                  you should check my channel for funny videos﻿  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine clean text\n",
    "processed_psy = psy_clean.copy()\n",
    "processed_psy['processed_comments'] = processed_psy['CONTENT'].apply(remove_punctuation)\n",
    "processed_psy[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "51bb2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this documentation to remove stop words: https://pythonspot.com/nltk-stop-words/#google_vignette\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d206b44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['check', '\\ufeff']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create function to tokenize words and remove stop words & stem words\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokenize_text(new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "71160a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                 [huh, anyway, check, youtub, channel, kobyoshi]\n",
       "1    [hey, guy, check, new, channel, first, vid, us, monkey, im, monkey, white, shirtpleas, leav, like, comment, pleas, subscrib]\n",
       "2                                                                                                          [test, say, murdevcom]\n",
       "3                                                                                           [shake, sexi, ass, channel, enjoy, ﻿]\n",
       "4                                                                                                                      [check, ﻿]\n",
       "Name: processed_comments, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply tokenization function to processed_comments\n",
    "processed_psy_list = processed_psy['processed_comments'].apply(tokenize_text)\n",
    "processed_psy_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "731f2c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[huh, anyway, check, youtub, channel, kobyoshi]</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: kobyoshi02</td>\n",
       "      <td>1</td>\n",
       "      <td>huh anyway check out this youtube channel kobyoshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hey, guy, check, new, channel, first, vid, us, monkey, im, monkey, white, shirtpleas, leav, like, comment, pleas, subscrib]</td>\n",
       "      <td>Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys check out my new channel and our first vid this is us the  monkeys im the monkey in the white shirtplease leave a like comment  and please subscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[test, say, murdevcom]</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>just for test i have to say murdevcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[shake, sexi, ass, channel, enjoy, ﻿]</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy  ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[check, ﻿]</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>check this out ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>[billion, view, there, planet, lol﻿]</td>\n",
       "      <td>How can this have 2 billion views when there's only me on the planet? LOL﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>how can this have  billion views when theres only me on the planet lol﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>[dont, im, watch, ﻿]</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>i dont now why im watching this in ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>[subscrib, call, duti, vid, give, away, goal, subs﻿]</td>\n",
       "      <td>subscribe to me for call of duty vids and give aways Goal-100 subs﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>subscribe to me for call of duty vids and give aways goal subs﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>[hi, guy, pleas, android, photo, editor, download, thank]</td>\n",
       "      <td>hi guys please my android photo editor download. thanks https://play.google.com/store/apps/details?id=com.butalabs.photo.editor﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>hi guys please my android photo editor download thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>[first, billion, view, thought, realli, cool, billion, half, came, see, stupid, first, billion, were﻿]</td>\n",
       "      <td>The first billion viewed this because they thought it was really cool, the  other billion and a half came to see how stupid the first billion were...﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>the first billion viewed this because they thought it was really cool the  other billion and a half came to see how stupid the first billion were﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   stemmed_tokens  \\\n",
       "0                                                                                 [huh, anyway, check, youtub, channel, kobyoshi]   \n",
       "1    [hey, guy, check, new, channel, first, vid, us, monkey, im, monkey, white, shirtpleas, leav, like, comment, pleas, subscrib]   \n",
       "2                                                                                                          [test, say, murdevcom]   \n",
       "3                                                                                           [shake, sexi, ass, channel, enjoy, ﻿]   \n",
       "4                                                                                                                      [check, ﻿]   \n",
       "..                                                                                                                            ...   \n",
       "345                                                                                          [billion, view, there, planet, lol﻿]   \n",
       "346                                                                                                          [dont, im, watch, ﻿]   \n",
       "347                                                                          [subscrib, call, duti, vid, give, away, goal, subs﻿]   \n",
       "348                                                                     [hi, guy, pleas, android, photo, editor, download, thank]   \n",
       "349                        [first, billion, view, thought, realli, cool, billion, half, came, see, stupid, first, billion, were﻿]   \n",
       "\n",
       "                                                                                                                                                                    CONTENT  \\\n",
       "0                                                                                                                  Huh, anyway check out this you[tube] channel: kobyoshi02   \n",
       "1    Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!   \n",
       "2                                                                                                                                    just for test I have to say murdev.com   \n",
       "3                                                                                                                          me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                                                                                                                                   watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "..                                                                                                                                                                      ...   \n",
       "345                                                                                              How can this have 2 billion views when there's only me on the planet? LOL﻿   \n",
       "346                                                                                                                              I don't now why I'm watching this in 2014﻿   \n",
       "347                                                                                                     subscribe to me for call of duty vids and give aways Goal-100 subs﻿   \n",
       "348                                        hi guys please my android photo editor download. thanks https://play.google.com/store/apps/details?id=com.butalabs.photo.editor﻿   \n",
       "349                  The first billion viewed this because they thought it was really cool, the  other billion and a half came to see how stupid the first billion were...﻿   \n",
       "\n",
       "     CLASS  \\\n",
       "0        1   \n",
       "1        1   \n",
       "2        1   \n",
       "3        1   \n",
       "4        1   \n",
       "..     ...   \n",
       "345      0   \n",
       "346      0   \n",
       "347      1   \n",
       "348      1   \n",
       "349      0   \n",
       "\n",
       "                                                                                                                                                processed_comments  \n",
       "0                                                                                                               huh anyway check out this youtube channel kobyoshi  \n",
       "1    hey guys check out my new channel and our first vid this is us the  monkeys im the monkey in the white shirtplease leave a like comment  and please subscribe  \n",
       "2                                                                                                                            just for test i have to say murdevcom  \n",
       "3                                                                                                                    me shaking my sexy ass on my channel enjoy  ﻿  \n",
       "4                                                                                                                                                 check this out ﻿  \n",
       "..                                                                                                                                                             ...  \n",
       "345                                                                                        how can this have  billion views when theres only me on the planet lol﻿  \n",
       "346                                                                                                                           i dont now why im watching this in ﻿  \n",
       "347                                                                                                subscribe to me for call of duty vids and give aways goal subs﻿  \n",
       "348                                                                                                        hi guys please my android photo editor download thanks   \n",
       "349             the first billion viewed this because they thought it was really cool the  other billion and a half came to see how stupid the first billion were﻿  \n",
       "\n",
       "[350 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_psy_df = pd.DataFrame(processed_psy_list)\n",
    "processed_psy_df = processed_psy_df.rename(columns={'processed_comments': 'stemmed_tokens'})\n",
    "psy_df = processed_psy_df.join(processed_psy, how = 'outer')\n",
    "psy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e9f2fc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['huh', 'anyway', 'check', 'youtub', 'channel', 'kobyoshi']</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: kobyoshi02</td>\n",
       "      <td>1</td>\n",
       "      <td>huh anyway check out this youtube channel kobyoshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['hey', 'guy', 'check', 'new', 'channel', 'first', 'vid', 'us', 'monkey', 'im', 'monkey', 'white', 'shirtpleas', 'leav', 'like', 'comment', 'pleas', 'subscrib']</td>\n",
       "      <td>Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys check out my new channel and our first vid this is us the  monkeys im the monkey in the white shirtplease leave a like comment  and please subscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['test', 'say', 'murdevcom']</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>just for test i have to say murdevcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['shake', 'sexi', 'ass', 'channel', 'enjoy', '\\ufeff']</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy  ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['check', '\\ufeff']</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>check this out ﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     stemmed_tokens  \\\n",
       "0                                                                                                       ['huh', 'anyway', 'check', 'youtub', 'channel', 'kobyoshi']   \n",
       "1  ['hey', 'guy', 'check', 'new', 'channel', 'first', 'vid', 'us', 'monkey', 'im', 'monkey', 'white', 'shirtpleas', 'leav', 'like', 'comment', 'pleas', 'subscrib']   \n",
       "2                                                                                                                                      ['test', 'say', 'murdevcom']   \n",
       "3                                                                                                            ['shake', 'sexi', 'ass', 'channel', 'enjoy', '\\ufeff']   \n",
       "4                                                                                                                                               ['check', '\\ufeff']   \n",
       "\n",
       "                                                                                                                                                                  CONTENT  \\\n",
       "0                                                                                                                Huh, anyway check out this you[tube] channel: kobyoshi02   \n",
       "1  Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!   \n",
       "2                                                                                                                                  just for test I have to say murdev.com   \n",
       "3                                                                                                                        me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4                                                                                                                                 watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      1   \n",
       "3      1   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                                              processed_comments  \n",
       "0                                                                                                             huh anyway check out this youtube channel kobyoshi  \n",
       "1  hey guys check out my new channel and our first vid this is us the  monkeys im the monkey in the white shirtplease leave a like comment  and please subscribe  \n",
       "2                                                                                                                          just for test i have to say murdevcom  \n",
       "3                                                                                                                  me shaking my sexy ass on my channel enjoy  ﻿  \n",
       "4                                                                                                                                               check this out ﻿  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psy_df['stemmed_tokens'] = psy_df['stemmed_tokens'].astype(str)\n",
    "psy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "59c0ee73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['love', 'much', 'also', 'gener', 'free', 'lead', 'auto', 'pilot', 'amp']</td>\n",
       "      <td>i love this so much. AND also I Generate Free Leads on Auto Pilot &amp;amp; You Can  Too! http://www.MyLeaderGate.com/moretraffic﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>i love this so much and also i generate free leads on auto pilot amp you can  too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['vote', 'sone', 'pleasewer', 'vipspleas', 'help', 'us', 'gtlt\\ufeff']</td>\n",
       "      <td>http://www.billboard.com/articles/columns/pop-shop/6174122/fan-army-face-off-round-3 Vote for SONES please....we're against vips....please help us.. &amp;gt;.&amp;lt;﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>vote for sones pleasewere against vipsplease help us gtlt﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['hey', 'guy', 'pleas', 'join', 'fight', 'help', 'abusedmistr', 'anim', 'fund', 'go', 'help', 'pay', 'vet', 'billsand', 'help', 'find', 'home', 'place', 'extra', 'emphasi', 'help', 'disabl', 'anim', 'one', 'otherwis', 'would', 'put', 'sleep', 'anim', 'organ', 'donat', 'pleas']</td>\n",
       "      <td>Hey guys! Please join me in my fight to help abused/mistreated animals! All  fund will go to helping pay for vet bills/and or helping them find homes! I  will place an extra emphasis on helping disabled animals, ones otherwise  would just be put to sleep by other animal organizations. Donate please. http://www.gofundme.com/Angels-n-Wingz﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys please join me in my fight to help abusedmistreated animals all  fund will go to helping pay for vet billsand or helping them find homes i  will place an extra emphasis on helping disabled animals ones otherwise  would just be put to sleep by other animal organizations donate please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['song\\ufeff']</td>\n",
       "      <td>http://psnboss.com/?ref=2tGgp3pV6L this is the song﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>this is the song﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['hey', 'everyon', 'watch', 'trailer']</td>\n",
       "      <td>Hey everyone. Watch this trailer!!!!!!!!  http://believemefilm.com?hlr=h2hQBUVB﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>hey everyone watch this trailer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                          stemmed_tokens  \\\n",
       "0                                                                                                                                                                                                              ['love', 'much', 'also', 'gener', 'free', 'lead', 'auto', 'pilot', 'amp']   \n",
       "1                                                                                                                                                                                                                 ['vote', 'sone', 'pleasewer', 'vipspleas', 'help', 'us', 'gtlt\\ufeff']   \n",
       "2  ['hey', 'guy', 'pleas', 'join', 'fight', 'help', 'abusedmistr', 'anim', 'fund', 'go', 'help', 'pay', 'vet', 'billsand', 'help', 'find', 'home', 'place', 'extra', 'emphasi', 'help', 'disabl', 'anim', 'one', 'otherwis', 'would', 'put', 'sleep', 'anim', 'organ', 'donat', 'pleas']   \n",
       "3                                                                                                                                                                                                                                                                         ['song\\ufeff']   \n",
       "4                                                                                                                                                                                                                                                 ['hey', 'everyon', 'watch', 'trailer']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                CONTENT  \\\n",
       "0                                                                                                                                                                                                                        i love this so much. AND also I Generate Free Leads on Auto Pilot &amp; You Can  Too! http://www.MyLeaderGate.com/moretraffic﻿   \n",
       "1                                                                                                                                                                                       http://www.billboard.com/articles/columns/pop-shop/6174122/fan-army-face-off-round-3 Vote for SONES please....we're against vips....please help us.. &gt;.&lt;﻿   \n",
       "2  Hey guys! Please join me in my fight to help abused/mistreated animals! All  fund will go to helping pay for vet bills/and or helping them find homes! I  will place an extra emphasis on helping disabled animals, ones otherwise  would just be put to sleep by other animal organizations. Donate please. http://www.gofundme.com/Angels-n-Wingz﻿   \n",
       "3                                                                                                                                                                                                                                                                                                  http://psnboss.com/?ref=2tGgp3pV6L this is the song﻿   \n",
       "4                                                                                                                                                                                                                                                                      Hey everyone. Watch this trailer!!!!!!!!  http://believemefilm.com?hlr=h2hQBUVB﻿   \n",
       "\n",
       "   CLASS  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      1   \n",
       "3      1   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                      processed_comments  \n",
       "0                                                                                                                                                                                                                     i love this so much and also i generate free leads on auto pilot amp you can  too   \n",
       "1                                                                                                                                                                                                                                             vote for sones pleasewere against vipsplease help us gtlt﻿  \n",
       "2  hey guys please join me in my fight to help abusedmistreated animals all  fund will go to helping pay for vet billsand or helping them find homes i  will place an extra emphasis on helping disabled animals ones otherwise  would just be put to sleep by other animal organizations donate please   \n",
       "3                                                                                                                                                                                                                                                                                      this is the song﻿  \n",
       "4                                                                                                                                                                                                                                                                      hey everyone watch this trailer    "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean 4 other datasets: kp_clean, lmfao_clean, eminem_clean, shakira_clean\n",
    "#kp:\n",
    "processed_kp = kp_clean.copy()\n",
    "#remove punctuation step\n",
    "processed_kp['processed_comments'] = processed_kp['CONTENT'].apply(remove_punctuation)\n",
    "#tokenization step\n",
    "processed_kp_list = processed_kp['processed_comments'].apply(tokenize_text)\n",
    "#convert to list\n",
    "processed_kp_df = pd.DataFrame(processed_kp_list)\n",
    "#rename col\n",
    "processed_kp_df = processed_kp_df.rename(columns={'processed_comments': 'stemmed_tokens'})\n",
    "#join stemmed, tokenized col\n",
    "kp_df = processed_kp_df.join(processed_kp, how = 'outer')\n",
    "#convert stemmed tokens to strings for model\n",
    "kp_df['stemmed_tokens'] = kp_df['stemmed_tokens'].astype(str)\n",
    "kp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2fb9c7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['href', 'best', 'part\\ufeff']</td>\n",
       "      <td>&lt;a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;amp;t=2m19s\"&gt;2:19&lt;/a&gt; best part﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>a href best part﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['wierd', 'funny\\ufeff']</td>\n",
       "      <td>wierd but funny﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>wierd but funny﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['hey', 'guy', 'im', 'humanbr', 'br', 'br', 'dont', 'want', 'human', 'want', 'sexi', 'fuck', 'giraffebr', 'br', 'br', 'alreadi', 'money', 'surgeri', 'elong', 'spinal', 'core', 'surgeri', 'chang', 'skin', 'pigment', 'everyth', 'els', 'like', 'post', 'other', 'root', 'dreambr', 'br', 'br', 'im', 'fuck', 'make', 'music', 'check', 'first', 'song', 'relnofollow', 'classothashtag', 'href']</td>\n",
       "      <td>Hey guys, I&amp;#39;m a human.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;But I don&amp;#39;t want to be a human, I want to be a sexy fucking giraffe.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;I already have the money for the surgery to elongate my spinal core, the surgery to change my skin pigment, and everything else! Like this post so others can root me on in my dream!!!!&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Im fucking with you, I make music, check out my first song! &lt;a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23giraffebruuh\"&gt;#giraffebruuh&lt;/a&gt;﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys im a humanbr br br but i dont want to be a human i want to be a sexy fucking giraffebr br br i already have the money for the surgery to elongate my spinal core the surgery to change my skin pigment and everything else like this post so others can root me on in my dreambr br br im fucking with you i make music check out my first song a relnofollow classothashtag href</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['parti', 'rocklolwho', 'want', 'shuffle\\ufeff']</td>\n",
       "      <td>Party Rock....lol...who wants to shuffle!!!﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>party rocklolwho wants to shuffle﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['parti', 'rock\\ufeff']</td>\n",
       "      <td>Party rock﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>party rock﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                       stemmed_tokens  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                      ['href', 'best', 'part\\ufeff']   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                            ['wierd', 'funny\\ufeff']   \n",
       "2  ['hey', 'guy', 'im', 'humanbr', 'br', 'br', 'dont', 'want', 'human', 'want', 'sexi', 'fuck', 'giraffebr', 'br', 'br', 'alreadi', 'money', 'surgeri', 'elong', 'spinal', 'core', 'surgeri', 'chang', 'skin', 'pigment', 'everyth', 'els', 'like', 'post', 'other', 'root', 'dreambr', 'br', 'br', 'im', 'fuck', 'make', 'music', 'check', 'first', 'song', 'relnofollow', 'classothashtag', 'href']   \n",
       "3                                                                                                                                                                                                                                                                                                                                                    ['parti', 'rocklolwho', 'want', 'shuffle\\ufeff']   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                             ['parti', 'rock\\ufeff']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                CONTENT  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                  <a href=\"http://www.youtube.com/watch?v=KQ6zr6kCPj8&amp;t=2m19s\">2:19</a> best part﻿   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      wierd but funny﻿   \n",
       "2  Hey guys, I&#39;m a human.<br /><br /><br />But I don&#39;t want to be a human, I want to be a sexy fucking giraffe.<br /><br /><br />I already have the money for the surgery to elongate my spinal core, the surgery to change my skin pigment, and everything else! Like this post so others can root me on in my dream!!!!<br /><br /><br />Im fucking with you, I make music, check out my first song! <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23giraffebruuh\">#giraffebruuh</a>﻿   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Party Rock....lol...who wants to shuffle!!!﻿   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Party rock﻿   \n",
       "\n",
       "   CLASS  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      1   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                           processed_comments  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                           a href best part﻿  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                            wierd but funny﻿  \n",
       "2  hey guys im a humanbr br br but i dont want to be a human i want to be a sexy fucking giraffebr br br i already have the money for the surgery to elongate my spinal core the surgery to change my skin pigment and everything else like this post so others can root me on in my dreambr br br im fucking with you i make music check out my first song a relnofollow classothashtag href  \n",
       "3                                                                                                                                                                                                                                                                                                                                                          party rocklolwho wants to shuffle﻿  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                 party rock﻿  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lmfao_clean\n",
    "processed_lmfao = lmfao_clean.copy()\n",
    "#remove punctuation step\n",
    "processed_lmfao['processed_comments'] = processed_lmfao['CONTENT'].apply(remove_punctuation)\n",
    "#tokenization step\n",
    "processed_lmfao_list = processed_lmfao['processed_comments'].apply(tokenize_text)\n",
    "#convert to list\n",
    "processed_lmfao_df = pd.DataFrame(processed_lmfao_list)\n",
    "#rename col\n",
    "processed_lmfao_df = processed_lmfao_df.rename(columns={'processed_comments': 'stemmed_tokens'})\n",
    "#join stemmed, tokenized col\n",
    "lmfao_df = processed_lmfao_df.join(processed_lmfao, how = 'outer')\n",
    "#convert stemmed tokens to strings for model\n",
    "lmfao_df['stemmed_tokens'] = lmfao_df['stemmed_tokens'].astype(str)\n",
    "lmfao_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1e6d9049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['love', 'girl', 'talk', 'xxx\\ufeff']</td>\n",
       "      <td>+447935454150 lovely girl talk to me xxx﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>lovely girl talk to me xxx﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['alway', 'end', 'come', 'back', 'songbr', '\\ufeff']</td>\n",
       "      <td>I always end up coming back to this song&lt;br /&gt;﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>i always end up coming back to this songbr ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['sister', 'receiv', 'new', 'relnofollow', 'classothashtag', 'href', 'youtub', 'view', 'right', 'thing', 'use', 'pimpmyview', 'com\\ufeff']</td>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23active\"&gt;#active&lt;/a&gt; youtube views Right now. The only thing she used was pimpmyviews. com﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>my sister just received over  new a relnofollow classothashtag href youtube views right now the only thing she used was pimpmyviews com﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cool\\ufeff']</td>\n",
       "      <td>Cool﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>cool﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['hello', 'iam', 'palastine\\ufeff']</td>\n",
       "      <td>Hello I&amp;#39;am from Palastine﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>hello iam from palastine﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               stemmed_tokens  \\\n",
       "0                                                                                                       ['love', 'girl', 'talk', 'xxx\\ufeff']   \n",
       "1                                                                                        ['alway', 'end', 'come', 'back', 'songbr', '\\ufeff']   \n",
       "2  ['sister', 'receiv', 'new', 'relnofollow', 'classothashtag', 'href', 'youtub', 'view', 'right', 'thing', 'use', 'pimpmyview', 'com\\ufeff']   \n",
       "3                                                                                                                              ['cool\\ufeff']   \n",
       "4                                                                                                         ['hello', 'iam', 'palastine\\ufeff']   \n",
       "\n",
       "                                                                                                                                                                                                     CONTENT  \\\n",
       "0                                                                                                                                                                  +447935454150 lovely girl talk to me xxx﻿   \n",
       "1                                                                                                                                                            I always end up coming back to this song<br />﻿   \n",
       "2  my sister just received over 6,500 new <a rel=\"nofollow\" class=\"ot-hashtag\" href=\"https://plus.google.com/s/%23active\">#active</a> youtube views Right now. The only thing she used was pimpmyviews. com﻿   \n",
       "3                                                                                                                                                                                                      Cool﻿   \n",
       "4                                                                                                                                                                             Hello I&#39;am from Palastine﻿   \n",
       "\n",
       "   CLASS  \\\n",
       "0      1   \n",
       "1      0   \n",
       "2      1   \n",
       "3      0   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                         processed_comments  \n",
       "0                                                                                                               lovely girl talk to me xxx﻿  \n",
       "1                                                                                              i always end up coming back to this songbr ﻿  \n",
       "2  my sister just received over  new a relnofollow classothashtag href youtube views right now the only thing she used was pimpmyviews com﻿  \n",
       "3                                                                                                                                     cool﻿  \n",
       "4                                                                                                                 hello iam from palastine﻿  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eminem_clean\n",
    "processed_eminem = eminem_clean.copy()\n",
    "#remove punctuation step\n",
    "processed_eminem['processed_comments'] = processed_eminem['CONTENT'].apply(remove_punctuation)\n",
    "#tokenization step\n",
    "processed_eminem_list = processed_eminem['processed_comments'].apply(tokenize_text)\n",
    "#convert to list\n",
    "processed_eminem_df = pd.DataFrame(processed_eminem_list)\n",
    "#rename col\n",
    "processed_eminem_df = processed_eminem_df.rename(columns={'processed_comments': 'stemmed_tokens'})\n",
    "#join stemmed, tokenized col\n",
    "eminem_df = processed_eminem_df.join(processed_eminem, how = 'outer')\n",
    "#convert stemmed tokens to strings for model\n",
    "eminem_df['stemmed_tokens'] = eminem_df['stemmed_tokens'].astype(str)\n",
    "eminem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2ed607db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['nice', 'song\\ufeff']</td>\n",
       "      <td>Nice song﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>nice song﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['love', 'song', '\\ufeff']</td>\n",
       "      <td>I love song ﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>i love song ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['love', 'song', '\\ufeff']</td>\n",
       "      <td>I love song ﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>i love song ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['let', 'make', 'first', 'femal', 'reach', 'one', 'billion', 'share', 'replay', '\\ufeff']</td>\n",
       "      <td>860,000,000 lets make it first female to reach one billion!! Share it and replay it! ﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>lets make it first female to reach one billion share it and replay it ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['shakira', 'best', 'worldcup\\ufeff']</td>\n",
       "      <td>shakira is best for worldcup﻿</td>\n",
       "      <td>0</td>\n",
       "      <td>shakira is best for worldcup﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              stemmed_tokens  \\\n",
       "0                                                                     ['nice', 'song\\ufeff']   \n",
       "1                                                                 ['love', 'song', '\\ufeff']   \n",
       "2                                                                 ['love', 'song', '\\ufeff']   \n",
       "3  ['let', 'make', 'first', 'femal', 'reach', 'one', 'billion', 'share', 'replay', '\\ufeff']   \n",
       "4                                                      ['shakira', 'best', 'worldcup\\ufeff']   \n",
       "\n",
       "                                                                                  CONTENT  \\\n",
       "0                                                                              Nice song﻿   \n",
       "1                                                                           I love song ﻿   \n",
       "2                                                                           I love song ﻿   \n",
       "3  860,000,000 lets make it first female to reach one billion!! Share it and replay it! ﻿   \n",
       "4                                                           shakira is best for worldcup﻿   \n",
       "\n",
       "   CLASS  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                         processed_comments  \n",
       "0                                                                nice song﻿  \n",
       "1                                                             i love song ﻿  \n",
       "2                                                             i love song ﻿  \n",
       "3   lets make it first female to reach one billion share it and replay it ﻿  \n",
       "4                                             shakira is best for worldcup﻿  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shakira_clean\n",
    "processed_shakira = shakira_clean.copy()\n",
    "#remove punctuation step\n",
    "processed_shakira['processed_comments'] = processed_shakira['CONTENT'].apply(remove_punctuation)\n",
    "#tokenization step\n",
    "processed_shakira_list = processed_shakira['processed_comments'].apply(tokenize_text)\n",
    "#convert to list\n",
    "processed_shakira_df = pd.DataFrame(processed_shakira_list)\n",
    "#rename col\n",
    "processed_shakira_df = processed_shakira_df.rename(columns={'processed_comments': 'stemmed_tokens'})\n",
    "#join stemmed, tokenized col\n",
    "shakira_df = processed_shakira_df.join(processed_shakira, how = 'outer')\n",
    "#convert stemmed tokens to strings for model\n",
    "shakira_df['stemmed_tokens'] = shakira_df['stemmed_tokens'].astype(str)\n",
    "shakira_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eee6ce",
   "metadata": {},
   "source": [
    "# NB Model Training\n",
    "\n",
    "### Models:\n",
    "[Psy dataset](#Psy-dataset)\n",
    "\n",
    "[Katy Perry dataset](#Katy-Perry-dataset)\n",
    "\n",
    "[LMFAO dataset](#LMFAO-dataset)\n",
    "\n",
    "[Eminem dataset](#Eminem-dataset)\n",
    "\n",
    "[Testing: Shakira dataset](#Testing-Shakira-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442110c",
   "metadata": {},
   "source": [
    "# Psy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "769dc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set features\n",
    "X_psy = psy_df['stemmed_tokens']\n",
    "y_psy = psy_df['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "78bc3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model in a function\n",
    "def naive_bayes_model(X, y):\n",
    "    #TF-IDF\n",
    "    tfidf = TfidfVectorizer()\n",
    "    #X_string = [' '.join(tokens) for tokens in X]\n",
    "    X_vectorized = tfidf.fit_transform(X)\n",
    "\n",
    "    #split data to evaluate performance over time across datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = .2, random_state = 5)\n",
    "\n",
    "    #initialize model\n",
    "    nb_model = MultinomialNB()\n",
    "\n",
    "    #fit model\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    #evaluate model\n",
    "    y_classification_preds = nb_model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_classification_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6097b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84        32\n",
      "           1       1.00      0.68      0.81        38\n",
      "\n",
      "    accuracy                           0.83        70\n",
      "   macro avg       0.86      0.84      0.83        70\n",
      "weighted avg       0.88      0.83      0.83        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_model(X_psy, y_psy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59432b6",
   "metadata": {},
   "source": [
    "I decided to split my first dataset to evaluate its initial performance and compare how it improves/worsens with the different datasets, and ultimately compare its performance to the last testing dataset. \n",
    "\n",
    "The model didn't perform too poorly; starting with the precision values, it seems it accurately predicted all spam comments as spam, whereas out of all non-spam predictions, only 73% were actually non-spam comments. It's interesting to see this split because the dataset had an even number of spam and nonspam comments. \n",
    "Next, the recall demonstrates that the model accurately identified all of the non-spam comments as non-spam, but was only able to identify 68% of the actual spam comments. Therefore the f1-scores of 84 and 81 show that the model has a better balance between recall and precision for the nonspam messages, and it has a slightly lower balance between the two metrics when it comes to spam messages. Overall, the accuracy of 83% shows that the model performed pretty well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d41626",
   "metadata": {},
   "source": [
    "# Katy Perry dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cc6b048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85        38\n",
      "           1       0.78      0.91      0.84        32\n",
      "\n",
      "    accuracy                           0.84        70\n",
      "   macro avg       0.85      0.85      0.84        70\n",
      "weighted avg       0.85      0.84      0.84        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set X and y features:\n",
    "X_kp = kp_df['stemmed_tokens']\n",
    "y_kp = kp_df['CLASS']\n",
    "\n",
    "\n",
    "#call model function\n",
    "naive_bayes_model(X_kp, y_kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f437cf",
   "metadata": {},
   "source": [
    "The model performed slightly better with the Katy Perry dataset, when comparing the overall accuracy of the model, this one improved by a point.\n",
    "The split of non-spam and spam messages remained the same since though this time the amounts were inversed. It is worth noting this because the precision for the non-spam messages improved from 73 to 91% meaning that from the nonspam comment predictions by the model, 91% were accurately not spam. For the spam messages, out of the spam predictions made by the model only 78% were actually spam. It is interesting to note that the model's recall reflects that the model only identified 79% of all the true nonspam comments, while identifying 91% of all the true spam messages. The inverse distribution of the testing and training classes is worth noting because of the model's reduction in precision from 100 to 78%. It is interesting though to note that the f1 scores for both the precision and recall metrics improved, and though the precision for spam messages lowered, the f1 score improved by 3 points altogether, which demonstrates the model's iterative learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93becd95",
   "metadata": {},
   "source": [
    "# LMFAO dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aab102e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89        40\n",
      "           1       0.90      0.92      0.91        48\n",
      "\n",
      "    accuracy                           0.90        88\n",
      "   macro avg       0.90      0.90      0.90        88\n",
      "weighted avg       0.90      0.90      0.90        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#features\n",
    "X_lmfao = lmfao_df['stemmed_tokens']\n",
    "y_lmfao = lmfao_df['CLASS']\n",
    "\n",
    "#call model\n",
    "naive_bayes_model(X_lmfao, y_lmfao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660eaa6",
   "metadata": {},
   "source": [
    "The model keeps improving! The overall accuracy this time went up from 84 to 90%. It is important to note that the size of the LMFAO dataset was significantly larger, which allowed the model to not only iterate on its learning thus far but also have a slightly larger training dataset to learn from as well. \n",
    "This model's precisions for both the nonspam and spam messages were identical, surprisingly, despite the uneven distribution of true spam and nonspam labels. Out of its spam and nonspam predictions, it accurately identified 90% of each category. However, its recall for the nonspam comments was a bit lower, showing less of a balance between the metrics which could be tied to having less nonspam comments to sort. The model was only able to identify 88% of all the nonspam messages while identifying 92% of all true spam comments. \n",
    "The f1 scores for both precision and recall improved significantly; in the previous iteration, the model's nonspam f1 score was 85% and it jumped by 4 points to 89%. For the spam category, the f1 score jumped from 84 to 91%, which is quite a significant improvement. We'll see if the model continues to improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f9195",
   "metadata": {},
   "source": [
    "# Eminem dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b8e71669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94        38\n",
      "           1       0.98      0.92      0.95        52\n",
      "\n",
      "    accuracy                           0.94        90\n",
      "   macro avg       0.94      0.95      0.94        90\n",
      "weighted avg       0.95      0.94      0.94        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#features\n",
    "X_eminem = eminem_df['stemmed_tokens']\n",
    "y_eminem = eminem_df['CLASS']\n",
    "\n",
    "#call model\n",
    "naive_bayes_model(X_eminem, y_eminem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3bbe4",
   "metadata": {},
   "source": [
    "The Eminem dataset was slightly larger still than the LMFAO dataset, so it is a bit unsurprising that the model continued to improve due to the learning its done thus far, along with having both more learning datapoints in training and more testing opportunities. \n",
    "The precision for the non-spam label remained the same as last time, meaning out of the non-spam model predictions only 90% were still accurately non spam, however, the model was able to identify 97% of all true nonspam comments in the data (per recall metric). On the other hand, the model did exceptionally well with the spam category, out of all its predictions for spam comments, 98% were indeed spam, and though its recall remained at 92%, meaning that it only correctly identified 92% of all the true spam comments, its overal accuracy jumped by 4 points from 90 to 94% nonetheless. \n",
    "I know the upcoming testing dataset is slightly smaller than the previous two datasets but I wonder since the model has continuously improved, whether the results will remain relatively high or potentially even improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e739e",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "This section will test my Naive Baye's model with the Shakira dataset, which has the split:\n",
    "\n",
    "- non-spam comments: 194\n",
    "\n",
    "- spam comments: 174"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a37c38",
   "metadata": {},
   "source": [
    "# Testing: Shakira dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f521499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89        39\n",
      "           1       0.96      0.77      0.86        35\n",
      "\n",
      "    accuracy                           0.88        74\n",
      "   macro avg       0.90      0.87      0.88        74\n",
      "weighted avg       0.89      0.88      0.88        74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#features\n",
    "X_shakira = shakira_df['stemmed_tokens']\n",
    "y_shakira = shakira_df['CLASS']\n",
    "\n",
    "#call model\n",
    "naive_bayes_model(X_shakira, y_shakira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d18f02",
   "metadata": {},
   "source": [
    "The model's performance worsened with the Shakira dataset I used as my testing set. This is a bit surprising because the overall score decreased by 6 points, which is pretty significant given that the model had been improving thus far. The model had a bit of a worse balance between the precision and recall metrics, out of its nonspam predictions only 83% were right, but it correctly found 97% of all the true nonspam comments, while out of its spam comment predictions 96% were correct, though it only found 77% of all the true spam comments in this testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c568b",
   "metadata": {},
   "source": [
    "# Overall Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034f050",
   "metadata": {},
   "source": [
    "it was surprising to see that the model had been continuously improving in its overall accuracy as well as improving its balance between precision and recall across the 4 training datasets I used (Psy, Katy Perry, LMFAO, and Eminem). I attribute some of the improvements to be due to the increasing size of the datasets -the first two had an even split across spam and nonspam comments and were of equal size, so the improvement on the second model can be attributed to the model's learning- the third and fourth datasets were larger than the previous two and allowed the model to access a larger pool of learning samples to build on top of its iterative learning up until that point.\n",
    "I wonder if my model worsened on Shakira's video due to the potential presence of comments in languages other than English. I would've expected the model to perform relatively the same, and not worsen as much as it did, so I attribute this discrepancy to potential anomalies in the data. Since Shakira is primarily a Spanish-speaking singer, I think perhaps some of the comments in the dataset were in Spanish and the model was not exposed to those comments before. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
